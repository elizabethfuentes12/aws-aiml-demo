 Machine Learning Best Practices for Public Sector Organizations AWS WhitepaperMachine Learning Best Practices for Public Sector OrganizationsAWS WhitepaperMachine Learning Best Practices for Public Sector Organizations: AWS WhitepaperCopyright  2023 Amazon Web Services, Inc. and/or its a.liates. All rights reserved.Amazon's trademarks and trade dress may not be used in connection with any product or service that is not Amazon's, in any manner that is likely to cause confusion among customers, or in any manner that disparages or discredits Amazon. All other trademarks not owned by Amazon are the property of their respective owners, who may or may not be a.liated with, connected to, or sponsored by Amazon. Machine Learning Best Practices for Public Sector Organizations AWS WhitepaperTable of ContentsAbstract and introductioniIntroduction1Challenges for public sector2Best Practices4Data Ingestion and Preparation4Data Ingestion4Data Preparation5Data quality6Model Training and Tuning6Model Selection6Model Training8Model Tuning8MLOps9Amazon SageMaker Projects9Amazon SageMaker Pipelines9AWS CodePipeline and AWS Lambda10AWS Step Functions Data Science Software Development Kit (SDK)10AWS MLOps Framework11Deploy Custom Deep Learning Models11Deploy ML at the edge11Management and Governance12Enable governance and control12Provision ML resources that meet policies12Operateenvironment with governance13Security and compliance14Compute and network isolation15Data Protection16Authentication and Authorization17Artifact and model management18Security compliance18Cost optimization18Prepare18Build19Train and Tune20Deploy and Manage21Bias and Explainability21Amazon SageMaker Debugger22Amazon SageMaker Clarify22SHAP and LIME (Local Interpretable Model-Agnostic Explanations) libraries:22Conclusion24Next Steps25References to Public Sector Use Cases26Contributors27Further Reading28Document history29Notices30AWS glossary31 iii Machine Learning Best Practices for Public Sector Organizations AWS WhitepaperIntroductionMachine Learning Best Practices for Public Sector OrganizationsPublication date: September 29, 2021 (Document history (p. 29))This whitepaper outlines some of the challenges for US public sector agencies in adoption and implementation of ML, and provides best practices to address these challenges. The target audience for this whitepaper includes executive leaders and agency IT Directors.IntroductionIn 2019, the White House issued an executive order promoting the use of trustworthy articial intelligence (AI) in the federal government. (Source: https://www.nitrd.gov/pubs/National-AI-RD-Strategy-2019.pdf) This order launched the American AI Initiative, a concerted e.ort to promote and protect AI technology and innovation in the United States. This executive order also laid the foundation, with broad guidelines and policies, for agencies on the design, development, acquisition, and the use of AI in government.Machine learning (ML) and deep learning (DL) are computer science elds derived from the discipline of AI. Collectively called ML in this whitepaper, these elds help modernize the government and ensure federal agencies are e.ectively delivering on their mission objectives on behalf of the American people. AI & ML can help government agencies solve complex problems with citizen services, public safety, healthcare, transportation, and other service verticals. To enable these capabilities, agencies are investing in AI & ML solutions, especially to improve mission e.ectiveness, make evidence-based decisions, and automate repetitive tasks. As an example, in 2018 the Defense Advanced Research Project Agency (DARPA) announced a multi-year investment of more than $2 billion in new and existing programs and called it the AI Next campaign. (Source: https://www.darpa.mil/work-with-us/ai-next-campaign) The National Science Foundation (NSF) invests more than $500 million in AI research annually. (Source: https://www.nsf.gov/cise/ai.jsp)However, several challenges remain within the US public sector regarding the broader adoption of ML initiatives. Organizations have stringent federal, state, and local security and compliance mandates including the Federal Risk and Authorization Management Program (FedRAMP), Department of Defense(DOD) Cloud Computing Security Requirements Guide (CC SRG), and theHealth Insurance Portability and Accountability Act(HIPAA), among others. These requirements include protecting sensitive citizen data, isolating environments from internet access, and the principles of least-privilege-access controls. Additionally, the ML lifecycle presents its own challenges in terms of data and model lifecycle management, including the bias within ML models that needs to be addressed to improve the trust with public.This whitepaper outlines some of the challenges for US public sector agencies in adoption and implementation of ML, and provides best practices to address these challenges. The target audience for this whitepaper includes executive leaders and agency IT Directors. You can get started on AI and ML by visiting Machine Learning on AWS, AWS Machine Learning Embark Program, or the Amazon Machine Learning Solutions Lab 1 Machine Learning Best Practices for Public Sector Organizations AWS Whitepaper Machine Learning Best Practices for Public Sector Organizations AWS WhitepaperChallenges for public sectorGovernment, education, and nonprot organizations face several challenges in implementing ML programs to accomplish their mission objectives. This section outlines some of the challenges in seven critical areas of an ML implementation. These are outlined as follows:1.Data Ingestion and Preparation. Identifying, collecting, and transforming data is the foundation for ML. The ability to extract data from di.erent types of data sources (ranging from at les to databases, structured and unstructured, real time and batch) can be challenging given the range of technologies found in public sector organizations. Once the data is extracted, it needs to be cataloged and organized so that it is available for consumption with the necessary approvals in compliance with public sector guidelines.2.Model Training and Tuning. There are hundreds of algorithms available for ML model training and tuning that solve various types of problems. One of the major challenges facing public sector organizations is the ability to create a common platform that provides these algorithms and the structure required for visibility and maintenance. Challenges also exist in optimizing model training performance with minimal resources without compromising on the quality of ML models.3.ML Operations (MLOps). Integrating ML into business operations, referred to as MLOps, requires signicant planning and preparation. One of the major hurdles facing government organizations is the ability to create a repeatable process for deployment that is consistent with their organizational best practices. Mechanisms need to be put in place to ensure scalability and availability, as well as recovery of the models in case of disasters. Another challenge is to e.ectively monitor the model in production to ensure that ML models do not lose their e.ectiveness due to introduction of new variables, changes in source data, or issues with source data.4.Management & Governance. Public sector organizations face increased scrutiny to ensure that public funds are being properly utilized to serve mission needs. As such, they need to provide increased visibility into monitoring and auditing ML workloads. Changes need to be tracked in several places, including data sources, data models, data transfer and transformation mechanisms, deployments and inference endpoints. A clear separation needs to be put in place between development and production workloads while enforcing separation of duties with appropriate approval mechanisms. In addition, any underlying infrastructure, software, and licenses need to be maintained and managed.5.Security & Compliance. Security and compliance of ML workloads is one of the biggest challenges facing public sector organizations. The sensitive nature of the work done by these organizations results in increased security requirements at all levels of an ML platform. This can be very challenging as data is spread across a large number of data sources, is constantly evolving, and is constantly sent across the network between data storage and compute platforms. Data is also transmitted between compute instances in the case of distributed learning. Last but not least is the alignment with the principles of least privilege and application of a consistent user authentication and authorization mechanism.6.Cost Optimization. Given the complexity of ML projects, and the amount of data, compute, and other software required to successfully manage a project, costs can quickly spiral out of control. The challenge facing public sector agencies is the need to account for the resources used, and to monitor the usage against specied cost centers and task orders. Not only do they need to track usage of resources, but they also need to be able to e.ectively manage the costs.7.Bias & Explainability. Given the impact of public sector organizations on the citizens, the ability to understand why an ML model makes a specic prediction becomes paramount  this is also known as ML explainability. Organizations are under pressure from policymakers and regulators to ensure that ML and data-driven systems do not violate ethics and policies, and do not result in potentially discriminatory behavior. In January 2020, the U.S. government published draft rules for the regulation of Articial Intelligence (AI) in the United States. These rules state that any government regulation of public sector AI must encourage reliable, robust, and trustworthy AI and these standards should be the overarching guiding theme. Demonstrating explainability is a signicant challenge because complex ML models are hard to understand and even harder to interpret and debug. Public sector organizations need to invest signicant time with appropriate tools, techniques, and mechanisms to demonstrate explainability and lack of bias in their ML models, which could be a deterrent to adoption. 2 3 Machine Learning Best Practices for Public Sector Organizations AWS WhitepaperData Ingestion and Preparation Machine Learning Best Practices for Public Sector Organizations AWS WhitepaperMachine Learning Best Practices for Public Sector Organizations AWS WhitepaperMachine Learning Best Practices for Public Sector Organizations AWS WhitepaperMachine Learning Best Practices for Public Sector Organizations AWS WhitepaperMachine Learning Best Practices for Public Sector Organizations AWS WhitepaperMachine Learning Best Practices for Public Sector Organizations AWS WhitepaperMachine Learning Best Practices for Public Sector Organizations AWS WhitepaperMachine Learning Best Practices for Public Sector Organizations AWS WhitepaperMachine Learning Best Practices for Public Sector Organizations AWS WhitepaperMachine Learning Best Practices for Public Sector Organizations AWS WhitepaperMachine Learning Best Practices for Public Sector Organizations AWS WhitepaperMachine Learning Best Practices for Public Sector Organizations AWS WhitepaperMachine Learning Best Practices for Public Sector Organizations AWS WhitepaperMachine Learning Best Practices for Public Sector Organizations AWS WhitepaperMachine Learning Best Practices for Public Sector Organizations AWS WhitepaperMachine Learning Best Practices for Public Sector Organizations AWS WhitepaperMachine Learning Best Practices for Public Sector Organizations AWS WhitepaperMachine Learning Best Practices for Public Sector Organizations AWS WhitepaperMachine Learning Best Practices for Public Sector Organizations AWS WhitepaperData PreparationData qualityModel SelectionModel Training MLOps AWS CodePipeline and AWS LambdaAWS MLOps FrameworkManagement and GovernanceOperateenvironment with governanceSecurity and compliance Compute and network isolationData ProtectionAuthentication and AuthorizationArtifact and model managementBuildTrain and TuneDeploy and ManageAmazon SageMaker DebuggerSHAP and LIME (Local Interpretable Model-Agnostic Explanations) libraries:Best PracticesAWS Cloud provides several fully-managed services that supply developers and data scientists with the ability to prepare, build, train, and deploy ML models. This section provides the best practices for using these services to address the challenges outlined earlier. The best practices are organized by the seven critical areas of an ML implementation described in the previous section.TopicsData Ingestion and Preparation  (p. 4)Model Training and Tuning (p. 6)MLOps  (p. 9)Management and Governance (p. 12)Security and compliance  (p. 14)Cost optimization  (p. 18)Bias and Explainability (p. 21)Data Ingestion and PreparationData ingestion and preparation involves processes in collecting, curating, and preparing the data for ML. Data ingestion involves collecting batch or streaming data in unstructured or structured format. Data preparation takes the ingested data and processes to a format that can be used with ML.Identifying, collecting, and transforming data is the foundation for ML. There is widespread consensus among ML practitioners that data preparation accounts for approximately 80% of the time spent in developing a viable ML model. (Source: https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/?sh=2fb540636f63) There are several challenges that public sector organizations face in this phase: First is the ability to connect to and extract data from di.erent types of data sources. Once the data is extracted, it needs to be cataloged and organized so that it is available for consumption, and there needs to be a mechanism in place to ensure that only authorized resources have access to the data. Mechanisms are also needed to ensure that source data transformed for ML is reviewed and approved for compliance with federal government guidelines.The AWS Cloud provides services that enable public sector customers to overcome challenges in data ingestion, data preparation, and data quality. These are further described as follows:Data IngestionThe AWS Cloud enables public sector customers to overcome the challenge of connecting to and extracting data from both streaming and batch data, as described in the following:Streaming Data. For streaming data, Amazon Kinesis and Amazon Managed Streaming for Apache Kafka (Amazon MSK) enable the collection, processing, and analysis of data in real time. Amazon Kinesis provides a suite of capabilities to collect, process, and analyze real-time, streaming data.Amazon Kinesis Data Streams (KDS) is a service that enables ingestion of streaming data. Producers of data push data directly into a stream, which consists of a group of stored data units called records. The stored data is available for further processing or storage as part of the data pipeline. Ingestion of streaming videos can be done using Amazon Kinesis Video Streams.This service can capture streams from millions of devices, and durably store, encrypt, and index video data for use in ML models. If data does not need to be stored for real-time processing, Amazon Kinesis Data Firehose is a service that can be used to deliver real-time streaming data to a chosen destination. For example, a data source could be a custom producer application and a destination could be Amazon Simple Storage Service (Amazon S3) or Amazon RedShift. If you already use Apache Kafka, you can use Amazon MSK, a fully managed service, to build and run applications that use Apache Kafkato process streaming data without needing Apache Kafka infrastructure management expertise.Batch Data. There are a number of mechanisms available for data ingestion in batch format. WithAWS Database Migration Services (AWS DMS), you can replicate and ingest existing databases while the source databases remain fully operational. The service supports multiple database sources and targets, including writing data directly to Amazon S3. AWS DataSyncis a data transfer service that simplies, automates, and accelerates moving and replicating data between on-premises storage systems such as network le system (NFS) and AWS storage services such asAmazon Elastic File System (EFS) and Amazon S3. You can use AWS Transfer Family for ingestion of data from at les using secure protocols such as Secure File Transfer Protocol (SFTP), File Transfer Protocol over SSL (FTPS), and File Transfer Protocol (FTP). For large amounts of data, you can use the AWS Snow Family for transferring data in bulk using secure physical appliances.Data PreparationOnce the data is extracted, it needs to be transformed and loaded into a data store for feeding into an ML model. It also needs to be cataloged and organized so that it is available for consumption, and also needs to enable data lineage for compliance with federal government guidelines. AWS Cloud provides three services that provide these mechanisms. They are:AWS Glue is a fully managed ETL (extract, transform and load) service that makes it simple and cost-e.ective to categorize, clean, enrich, and migrate data from a source system to a data store for ML. The AWS AWS Glue Data Catalog provides the location and schema of ETL jobs as well as metadata tables (where each table species a single source data store). A crawler can be set to automatically take inventory of the data in your data stores.ETL jobs in AWS Glue consist of scripts that contain the programming logic that performs the transformation. Triggers are used to initiate jobs either on a schedule or as a result of a specied event. AWS Glue Studio provides a graphical interface that enables visual composition of data transformation workows on AWS Glues Apache Spark-based serverless ETL engine. AWS Glue generates the code that's required to transform the data from source to target based on the source and target information provided. Custom scripts can also be provided in the AWS Glue console or API to transform and process the data.In addition, AWS Glue DataBrew, a visual data preparation tool, can be used to simplify the process of cleaning and normalizing the data. It comes with hundreds of data transformations that can be used quickly to prepare data for ML without having to write your own transformation scripts.AWS Glue also features the ability to integrate with Amazon SageMaker. Amazon SageMaker is a comprehensive service that provides purpose-built tools for every step of ML development and implementation. In AWS Glue, you can create a development endpoint and then create a SageMaker notebook to help develop your ETL and ML scripts. A development endpoint allows you to iteratively develop and test your ETL scripts using the AWS Glue console or API.Amazon SageMaker Data Wrangler is a service that enables the aggregation and preparation of data for ML and is directly integrated into Amazon SageMaker Studio. Both Amazon Data Wrangler and Amazon SageMaker Studio are features of the Amazon SageMaker service. Data Wrangler contains hundreds of built-in transformations to quickly normalize, transform, and combine features without having to write any code. Using the Data Wrangler user interface, you can view table summaries, histograms, and scatter plots.Amazon EMR: Many organizations use Spark for data processing and other purposes such as for a data warehouse. 
 These organizations already have a complete end-to-end pipeline in Spark and also the skillset and inclination to run a persistent Spark cluster for the long term. In these situations, Amazon EMR, a managed service for Hadoop-ecosystem clusters, can be used to process data. Amazon EMR reduces the need to set up, tune, and maintain clusters.Amazon EMR also features other integrations with Amazon SageMaker, for example, to start a SageMaker model training job from a Spark pipeline in Amazon EMR.Data qualityData that is obsolete or inaccurate not only causes issues in developing accurate ML models, but can signicantly erode stakeholder and public trust. Public sector organizations need to ensure that data ingested and prepared for ML is of the highest quality by establishing a well-dened data quality framework. See How to Architect Data Quality on the AWS Cloud for an example on how you can set up a data quality framework on the AWS Cloud.Model Training and TuningModel Training and Tuning involves the selection of a ML model that is appropriate for the use case, followed by training and tuning of the ML model.One of the major challenges facing the public sector is the ability for team members to apply a consistent pattern or framework for working with multitudes of options that exist in this space. Di.erent teams use di.erent technologies and it is challenging to bring these into a uniform environment for increased visibility and tracking. For example, some teams may be using Python, while some other teams use R. Some teams may have standardized on TensorFlow, whereas other teams may have standardized on PyTorch. Challenges also exist in optimizing model training performance, input data formats, and distributed training. A signicant amount of time is spent on ne tuning a model to achieve the expected performance.The AWS Cloud enables public sector customers to overcome challenges in model selection, training, and tuning as described in the following.Model SelectionAmazon SageMaker provides the exibility to select from a wide number of options using a consistent underlying platform.Programming Language. Amazon SageMaker notebook kernels provide the ability to use both Python, as well as R, natively. The Amazon SageMaker Python SDK provides open-source Python APIs and containers to train and deploy models in SageMaker. To use coding languages such as Stan or Julia, a Docker image can be created and brought into SageMaker for model training and inference (see Figure 3 below for more details on this option). To use programming languages like C++ or Java, custom images on Amazon ECS/EKS can be used to perform model training.Built-in algorithms: Amazon SageMaker Built-in Algorithms provides several built-in algorithms covering di.erent types of ML problems. These algorithms are already optimized for speed, scale, and accuracy. Additionally, for classication or regression with tabular data, SageMaker Autopilot can be used to automatically explore data, select algorithms relevant to the problem type, and prepare the data to facilitate model training and tuning. AutoML ranks all of the optimized models tested by their performance and nds out the best performing model. The AutoML approach is especially useful for application programmers who are new to ML.Script Mode: For experienced ML programmers who are comfortable with using their own algorithms, Amazon SageMaker provides the option to write your custom code (script) in a text le with a.pyextension (see Figure 1).Diagram showing custom training script on a supported frameworkFigure 1: Script ModeThis option is known as script mode and the custom code can be written using any SageMaker supported framework. Code needs to be prepared and packaged in a Python le (.py extension), adding in some training environment variables as input arguments. Code that requires Python packages hosted on PyPi can be listed in a requirement.txt le and included in the code directory.Use a custom Docker image: ML programmers may be using algorithms that are not included in aSageMaker supported framework, not hosted on PyPi, or written in a language like Stan and Julia. In these cases, the training of the algorithm and serving of the model can be done using a custom Docker image (see Figure 2 below).Diagram showing bring your own containerFigure 2: Bring your own containerFor more information on custom Docker images in SageMaker, seeUsing Docker containers with SageMakerModel TrainingAmazon SageMaker provides a number of built-in options for optimizing model training performance, input data formats, and distributed training.Data parallel: ML training processes go through an entire dataset in one training cycle called an epoch. It is common to have multiple training iterations per epoch. When the training dataset is big, each epoch becomes time consuming. In these situations, SageMakers distributed data parallel librarycan be considered for running training jobs in parallel. The library optimizes the training job for AWS network infrastructure and Amazon EC2 instance topology, and takes advantage of gradient updates to communicate between nodes with a custom algorithm.Pipe mode: Pipe mode accelerates the ML training process: instead of downloading data to the local Amazon EBS volume prior to starting the model training, Pipe mode streams data directly from S3 to the training algorithm while it is running. This enables the training job to start sooner, nish quicker, and need less disk space.Incremental training: Amazon SageMaker supports incremental training to train a new model from an existing model artifact, to save both training time and resources. Incremental training may be considered when there are publicly available pre-trained models related to the ML use case. It can also be considered if an expanded dataset contains an underlying pattern that was not accounted in previous models, or to resume a stopped training job.Model Parallel training: Sometimes ML models are too large to t into GPU memory in a training process. In these situations, Amazon SageMakers distributed model parallel library can be used to automatically and e.ciently split a model across multiple GPUs and instances and coordinate model training.Model TuningAmazon SageMaker provides automatic hyperparameter tuning to nd the best version of a model in an e.cient manner, enabling public sector organizations to judiciously use their resource on other activities. SageMaker hyperparameter tuning runs many training jobs on a dataset using specied ranges of hyperparameters. It then chooses the hyperparameter values that result in a model that performs the best, as measured by a selected metric. The following best practices ensure a better tuning result:Limit the number of hyperparameters: Up to 20 hyperparameters can be simultaneously specied to optimize a tuning job. However, limiting the search to a much smaller number is likely to give better results, as this can reduce the computational complexity of a hyperparameter tuning job. Moreover, a smaller number of hyperparameters provides better understanding of how a specic hyperparameter would a.ect the model performance.Choose hyperparameter ranges appropriately: The range of values for hyperparameters can signicantly a.ect the success of hyperparameter optimization. Better results are obtained by limiting the search to a small range of values. If the best metric values within a subset of the possible range are already known, consider limiting the range to that subset.Pay attention to scales for hyperparameters: During hyperparameter tuning, SageMaker attempts to gure out if hyperparameters are log-scaled or linear-scaled. 
 Initially, it assumes that hyperparameters are linear-scaled. If they are in fact log-scaled, it might take some time for SageMaker to discover that fact. Directly setting hyperparameters as log-scaled when theyre already known could improve hyperparameter optimization.Set the best number of concurrent training jobs: Running more hyperparameter tuning jobs concurrently gets more work done quickly, but a tuning job improves only through successive rounds of experiments. Typically, running one training job at a time achieves the best results with the least amount of compute time.Report the wanted objective metric for tuning when the training job runs on multiple instances:When a training job runs on multiple instances, hyperparameter tuning uses the last-reported objective metric value from all instances of that training job as the value of the objective metric for that training job. Therefore, distributed training jobs should be designed such that the objective metric reported is the one that is needed.Enable early stopping for hypermeter tuning job: Early stopping helps reduce compute time and helps avoid overtting the model. It stops the training jobs that a hyperparameter tuning job launches early when they are not improving signicantly as measured by the objective metric.Run a warm start using previous tuning jobs: Use a warm start for ne-tuning previous hyperparameter tuning jobs. A warm start uses information from the previous hyperparameter tuning jobs to increase the performance of the new hyperparameter tuning job by making the search for the best combination of hyperparameters more e.cient.MLOpsMLOps is the discipline of integrating ML workloads into release management, Continuous Integration / Continuous Delivery (CI/CD), and operations.One of the major hurdles facing government organizations is the ability to create a repeatable process for deployment that is consistent with their organizational best practices. Using ML models in software development makes it di.cult to achieve versioning, quality control, reliability, reproducibility, explainability, and audibility in that process. This is due to the number of changing artifacts to be managed in addition to the software code, such as the datasets, the ML models, the parameters and hyperparameters used by such models, and the size and portability of such artifacts can be orders of magnitude higher than the software code. In addition, di.erent teams might own di.erent parts of the process; data engineers might be building pipelines to make data accessible, while data scientists can be researching and exploring better models. ML engineers or developers have to work on integrating the models and releasing them to production. When these groups work independently, there is a high risk of creating friction in the process and delivering suboptimal results.AWS Cloud provides a number of di.erent options that solve these challenges, either by building an MLOps pipeline from scratch or by using managed services.Amazon SageMaker ProjectsA SageMaker project is an Service Catalog provisioned product that enables creation of an end-to-end ML solution. By using a SageMaker project, teams of data scientists and developers can work together on ML business problems. SageMaker projects use MLOps templates that automate the model building and deployment pipelines using CI/CD. SageMaker-provided templates can be used to provision the initial setup required for a complete end-to-end MLOps system including model building, training, and deployment. Custom templates can also be used to customize the provisioning of resources.Amazon SageMaker PipelinesSageMaker Pipelines is a purpose-built, CI/CD service for ML. SageMaker Pipelines brings CI/CD practices to ML, such as maintaining parity between development and production environments, version control, on-demand testing, and end-to-end automation, helping scale ML throughout the organization. Pipelines is integrated with SageMaker Python SDK as well as SageMaker Studio for visualization and management of workows. With the SageMaker Pipelines model registry, model versions can be stored in a central repository for easy browsing, discovery, and selection of the right model for deployment based on business requirements. Pipelines provide the ability to log each step within the ML workow for a complete audit trail of model components such as training data, platform congurations, model parameters, and learning gradients. Audit trails can be used to recreate models and help support compliance requirements.AWS CodePipeline and AWS LambdaFor AWS programmers and teams that are already working with CodePipeline for deployment of other workloads, the option exists to utilize the same workows for ML. Figure 3 below represents a reference pipeline for deployment on AWS.Reference Architecture CI/CD Pipeline for ML on AWSFigure 3: Reference Architecture CI/CD Pipeline for ML on AWSSee Build a CI/CD pipeline for deploying custom machine learning models using AWS services for details on the reference architecture and implementation.AWS Step Functions Data Science Software Development Kit (SDK)The AWS Step Functions Data Science SDK is an open-source Python library that allows data scientists to create workows that process and publish ML models using SageMaker and Step Functions. This can be used by teams that are already comfortable using Python and AWS Step Functions. The SDK provides the ability to copy workows, experiment with new options, and then put the rened workow in production. The SDK can also be used to create and visualize end-to-end data science workows that perform tasks such as data pre-processing on AWS Glue and model training, hyperparameter tuning, and endpoint creation on Amazon SageMaker. Workows can be reused in production by exportingAWS CloudFormation (infrastructure as code)templates.AWS MLOps FrameworkFigure 4 below illustrates an AWS solution that provides an extendable framework with a standard interface for managing ML pipelines.Diagram showing AWS MLOps FrameworkFigure 4: AWS MLOps FrameworkThe solution provides a ready-made template to upload trained models (also referred to as abring your own model), congure the orchestration of the pipeline, and monitor the pipeline's operations.Deploy Custom Deep Learning ModelsIn addition to Amazon SageMaker, AWS also provides the option to deploy custom code on virtual machines using Amazon EC2, and containers using self-managed Kubernetes on Amazon EC2, Amazon Elastic Container Service (Amazon ECS) and Amazon Elastic Kubernetes Service (Amazon EKS). AWS Deep Learning AMIs can be used to accelerate deep learning by quickly launching Amazon EC2 instances that are pre-installed with popular deep learning frameworks. AWS Deep Learning Containers are Docker images pre-installed with deep learning frameworks to deploy optimized ML environments. For an example of how to deploy custom deep learning models, see Deploy Deep Learning Models on Amazon ECS.Deploy ML at the edgeTraining your ML models requires powerful compute infrastructure available in the cloud. However, making inferences against these models typically requires far less computational power. In some cases, such as with edge devices, inferencing needs to occur even when there is limited or no connectivity to the cloud. Mining elds are an example of this type of use case. To make sure that an edge device can respond quickly to local events, it is critical that you can get inference results with low latency.AWS IoT Greengrass enables ML inference locally using models that are created, trained, and optimized in the cloud using Amazon SageMaker, AWS Deep Learning AMI, or AWS Deep Learning Containers, and deployed on the edge devices.Performing inference locally on connected devices running AWS IoT Greengrass reduces latency and cost. Instead of sending all device data to the cloud to perform ML inference and make a prediction, you can run inference directly on the device. As predictions are made on these edge devices, you can capture the results and analyze them to detect outliers. Analyzed data can then be sent back to the cloud, where it can be reclassied and tagged to improve the ML model. For example, you can build a predictive model in Amazon SageMaker for scene detection analysis, optimize it to run on any camera, and then deploy it to send an alert when suspicious activity occurs. Data gathered from the inference running on AWS IoT Greengrass can be sent back to Amazon SageMaker, where it can be tagged and used to continuously improve the quality of the ML models. See Machine Learning at the Edge: Using and Retraining Image Classication Models with AWS IoT Greengrass (Part 1) for more details.Management and GovernancePublic sector organizations face increased scrutiny to ensure that funds are properly utilized to serve mission needs. As such, ML workloads need to provide increased visibility for monitoring and auditing. Changes need to be tracked in several places, including data sources, data models, data transfer processes and transformation processes, and deployment endpoints and inference endpoints. A clear separation needs to be put in place between development and production workloads, while enforcing separation of duties with appropriate approval mechanisms. In addition, any underlying infrastructure, software, and licenses need to be maintained and managed. This section highlights several AWS services and associated best practices to address these management and governance challenges.Enable governance and controlAWS Cloud provides several services that enable governance and control. These include:AWS Control Tower. Setup and governance can be complex and time consuming for organizations with multiple AWS accounts and teams. AWS Control Tower creates a landing zone that consists of a predened structure of accounts using AWS Organizations, the ability to create accounts usingService Catalog, enforcement of compliance rules called guardrails using Service Control Policies, and detection of policy violations using AWS Cong. (See the Cross-account deployments in an AWS Control Tower environment blog for details on how to set up Control Tower)AWS License Manager. Public sector organizations may have existing software with their own licenses being used for various tasks in ML such as ETL. AWS License Manager can be used to track this software obtained from the AWS Marketplace and keep a consolidated view of all licenses. AWS License Manager enables sharing of licenses with other accounts in the organization.Resource Tagging. Organizing AI/ML resources can be done using tags. Each tag is a simple label consisting of a customer-dened key and an optional value that can make it easier to manage, search for, and lter resources by purpose, owner, environment, or other criteria. Automated tools such asAWS Resource Groupsand theResource Groups Tagging APIenable programmatic control of tags, making it easier to automatically manage, search, and lter tags and resources. To make the most e.ective use of tags, organizations should create business-relevant tag groupings to organize their resources along technical, business, and security dimensions.Provision ML resources that meet policiesAWS Cloud provides several services that enable consistent and repeatable provisioning of ML resources per organization policies.AWS CloudFormation. A successful AI/ML solution may involve resources from multiple services. Deploying and managing these resources one by one can be time-consuming and inconvenient. AWS CloudFormation provides a mechanism to model a collection of related AWS and third-party resources, provision them quickly and consistently, and manage them throughout their lifecycles, by treating infrastructure as code.AWS Cloud Development Kit (AWS CDK) (CDK). Many team members prefer to work in their own language to dene the infrastructure, as opposed to using JSON and YAML. The AWS CDK, an open-source software development framework, allows teams to dene cloud infrastructure in code directly in supported programming languages (i.e., TypeScript, JavaScript, Python, Java, and C#). CDK denes reusable cloud components known as Constructs, and composes them together into Stacks and Apps. The constructs are synthesized into CloudFormation at the time of deployment.Service Catalog. Deploying and setting up ML workspaces for a group or di.erent groups of people is always a big challenge for public sector organizations. Service Catalog provides a solution for this problem. It enables the central management of commonly deployed IT services, and achieves consistent governance and meets compliance requirements. End users can quickly deploy only the approved IT services they need, following the constraints set by the organization. For example, Service Catalog can be used with Amazon SageMaker notebooks to provide end users a template to quickly deploy and set up their ML Workspace. The following diagram shows how Service Catalog ensures two separate workows for cloud system administrators and data scientists or developers who work with Amazon SageMaker.Setting up ML workspace using Service CatalogFigure 5: Setting up ML workspace using Service CatalogBy leveraging Service Catalog, cloud administrators are able to dene the right level of controls and enforce data encryption along with centrally-mandated tags for any AWS service used by various groups. At the same time, data scientists can achieve self-service and a better security posture by simply launching an Amazon SageMaker notebook instance through Service Catalog.Operateenvironment with governanceAWS Cloud provides several services that enable the reliable operation of the ML environment.Amazon CloudWatch is a monitoring and observability service used to monitor resources and applications run on AWS in real time. Amazon SageMaker has built-in Amazon CloudWatch monitoring and logging to manage production compute infrastructure and perform health checks, apply security patches, and conduct other routine maintenance. For a complete list of metrics that can be monitored, refer to the Monitor Amazon SageMaker with Amazon CloudWatch section of the SageMaker user guide.Amazon EventBridge is a serverless event bus service that can monitor status change events in Amazon SageMaker. EventBridge enables automatic responses to events such as a training job status change or endpoint status change. Events from SageMaker are delivered to EventBridge in near real time. Simple rules can be written to indicate which events are of interest, and what automated actions to take when an event matches a rule.SageMaker Model Monitor can be used to continuously monitor the quality of ML models in production. Model Monitor can notify team members when there are deviations in the model quality. Early and proactive detection of these deviations enables corrective actions, such as retraining models, auditing upstream systems, or xing quality issues without having to monitor models manually or build additional tooling. The model monitor provides various types of monitoring, including data quality drift, model quality drift, bias drift, and feature attribution drift. For a sample notebook with the full end-to-end workow for Model Monitor, see theIntroduction to Amazon SageMaker Model Monitor or see Monitoring in-production ML models at large scale using Amazon SageMaker Model Monitor, which outlines how to monitor ML models in production at scale.AWS CloudTrail. Amazon SageMaker is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in SageMaker. CloudTrail captures all API calls for SageMaker. The calls captured include actions from the SageMaker console and code calls to the SageMaker API operations. Continuous delivery of CloudTrail events can be delivered to an Amazon S3 bucket, including events for SageMaker. Every event or log entry contains information about who generated the request.Security and compliancePublic sector organizations have a number of security challenges and concerns with hosting ML workloads in the cloud as these applications can contain sensitive customer data  this includes personal information or proprietary information that must be protected over the entire data lifecycle. The specic concerns also include protecting the network and underlying resources such as compute, storage and databases; user authentication and authorization; logging, monitoring and auditing. These objectives are summarized in Figure 6 below.Diagram showing Security and Compliance objectives for hosting public sector ML          workloadsFigure 6: Security and Compliance objectives for hosting public sector ML workloadsThis subsection provides best practices and guidelines to address some of these security and compliance challenges.Compute and network isolationOne of the major requirements with many public sector ML projects is the ability to keep the environments, data and workloads secure and isolated from internet access. These can be achieved using the following methods:Provision ML components in an isolated VPC with no internet access: SageMaker components including the studio, notebooks, training jobs and hosting instances can be provisioned in an isolated VPC with no internet access. Tra.c can be restricted from accessing the internet by launching SageMaker Studio in a Virtual Private Cloud (VPC) of choice. This allows ne-grained control of the network access and internet connectivity of SageMaker Studio notebooks. Direct internet access can be disabled to add an additional layer of security.To disable direct internet access, specify theVPC onlynetwork access type when onboarding to Studio. The same concept can be applied to SageMaker notebooks by choosing to launch the notebook instance in a VPC to restrict which tra.c can go through the public Internet. When launched with the VPC attached, the notebook instance can be congured either with or without direct internet access. Tra.c to public endpoints such as S3 or SageMaker APIs can be congured to traverse over VPC endpoints to ensure that the tra.c stays within the AWS network. Please refer to Building secure ML environments with Amazon SageMaker for further details.Use VPC end-point and end-point policies to further limit access: AWS resources can be directly connected with public endpoints such as S3, CloudWatch, and SageMaker API / SageMaker Runtime through an interface endpoint in the VPC instead of connecting over the internet. When a VPC interface endpoint is used, communication between the VPC and the SageMaker API or Runtime is entirely and securely within the AWS network. VPC endpoint policies can be congured to further limit access based on who can perform actions, what actions can be performed, and the resources on which these actions can be performed. As an example, access to an S3 bucket can be restricted only to a specic SageMaker studio domain or set of users, and each studio domain can be restricted to have access only to a specic S3 bucket (see Securing Amazon SageMaker Studio connectivity using a private VPC, which outlines how to secure SageMaker studio connectivity using a private VPC). Figure 7 below outlines an architecture diagram that represents how to set up SageMaker studio using a private VPC.Diagram showing SageMaker Studio in a private VPCFigure 7: SageMaker Studio in a private VPCAllow access from only within the VPC: An IAM policy can be created to prevent users outside the VPC from accessing SageMaker Studio or SageMaker notebooks over the internet. This ensures access to only connections made from within the VPC. As an example, this policy can help restrict connections made only through specic VPC endpoints or a specic set of source IP addresses. This policy can be added to every user, group, or role used to access Studio or Jupyter notebooks.Intrusion detection and prevention: AWS Gateway Load Balancer (GWLB) can be used to deploy, scale, and manage the availability of third-party virtual appliances such asrewalls,  intrusion detection and prevention systems,and deep packet inspection systems in the cloud.GWLB allows custom logic or third party o.ering into any networking path for AWS where inspection is needed and the corresponding action is taken on packets. For example, a simple application can be developed to check if there is any unencrypted tra.c or TLS1.0/TLS1.1 tra.c between VPCs. 
 Additionally, AWS Partner NetworkandAWS Marketplacepartners can o.er their virtual appliances as a service to AWS customers without having to solve the complex problems of scale, availability, and service delivery. Please refer to Introducing AWS Gateway Load Balancer  Easy Deployment, Scalability, and High Availability for Partner Appliances for further details on GWLB.Additional security to allow access to resources outside your VPC: If access is needed to an AWS service that does not support interface VPC endpoints, or to a resource outside of AWS, a NAT gateway needs to be created and security groups need to be congured to allow outbound connections. Additionally, AWS Network Firewall can be used to lter outbound tra.c, for example, to specic GitHub repositories. AWS Network Firewall supports inbound and outbound web ltering for unencrypted web tra.c. For encrypted web tra.c, Server Name Indication (SNI) is used for blocking access to specic sites. In addition, AWS Network Firewall can lter fully qualied domain names (FQDN).Data ProtectionProtect data at rest: AWS Key Management service (KMS) can be used to encrypt ML data, studio notebooks and SageMaker notebook instances. SageMaker uses KMS keys (formerly CMKs) by default. KMS keys can be used to get more control on encryption and key management. For studio notebooks, the ML-related data is primarily stored in multiple locations. An S3 bucket hosts notebook snapshots and metadata, EFS volumes contain studio notebook and data les, and EBS volumes are attached to the instance that the notebook runs on. KMS can be used for encrypting all these storage locations. Encryption keys can be specied to encrypt the volumes of all Amazon EC2-based SageMaker resources, such as processing jobs, notebooks, training jobs, and model endpoints. FIPS endpoints can be used if FIPS 140-2 validated cryptographic modules are required to access AWS through a command line interface or an API.Protect data in transit: To protect data in transit, AWS makes extensive use of HTTPS  communication for its APIs. Requests to the SageMaker API and console are made over a  secure (SSL) connection. In addition to passing all API calls through a TLS-encrypted  channel, AWS APIs also require that requests are signed using theSignature Version  4signing process. This process uses client access keys to sign every API request,  adding authentication information as well as preventing tampering of the request in  flight.Additionally, communication between instances in a distributed training job can be  further protected and another level of security can be added to protect your training  containers and data by configuring a private VPC. SageMaker can be instructed toencrypt  inter-node communicationautomatically for the training job. The data passed between  nodes is then passed over an encrypted tunnel without the algorithm having to take on  responsibility for encrypting and decrypting the data.Secure shared notebook instances: SageMaker notebook instances are designed to work best for individual users. They give data scientists and other users the most power for managing their development environment. A notebook instance user has root access for installing packages and other pertinent software. The recommended best practice is to use IAM policies when granting individuals access to notebook instances that are attached to a VPC that contains sensitive information. For example, allow only specic users access to a notebook instance with an IAM policy.Authentication and AuthorizationAWS IAM enables control of access to AWS resources. IAM administrators control who can be authenticated (signed in) and authorized (have permissions) to use SageMaker resources. IAM can help create preventive controls for many aspects of your ML environment, including access to Amazon SageMaker resources, data in Amazon S3, and API endpoints. AWS services can be accessed using a RESTful API, and every API call is authorized by IAM. Explicit permissions can be granted through IAM policy documents, which specify the principal (who), the actions (API calls), and the resources (such as Amazon S3 objects) that are allowed, as well as the conditions under which the access is granted. Access can be controlled by creating policies and attaching them to IAM identities or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, denes their permissions. Two common ways to implement least privilege access to the SageMaker environments areidentity-based policiesandresource-based policies:Identity-based policiesare attached to a user, group, or role. These policies specify what that identity can do. For example, by attaching the AmazonSageMakerFullAccessmanaged policy to an IAM role for data scientists, they are granted full access to the SageMaker service for model development work.Resource-based policiesare attached to a resource. These policies specify who has access to the resource, and what actions can be performed on it. For example, a policy can be attached to anAmazon Simple Storage Service (Amazon S3)bucket, granting read-only permissions to data scientists accessing the bucket from a specic VPC endpoint. Another typical policy conguration for S3 buckets is to deny public access, to prevent unauthorized access to data.Please refer to Conguring Amazon SageMaker Studio for teams and groups with complete resource isolation, which outlines how to congure access control for teams or groups within Amazon SageMaker Studio usingattribute-based access control(ABAC). ABAC is a powerful approach that can be utilized to congure Studio so that di.erent ML and data science teams have complete isolation of team resources.AWS Single Sign-On (AWS SSO) can also be used for user authentication with an external identity provider such as Ping identity or Okta. Please refer to Onboarding Amazon SageMaker Studio with AWS SSO and Okta Universal Directory, which outlines how to onboard SageMaker Studio with SSO and Okta universal directory.Artifact and model managementThe recommended best practice is to use version control to track code or other model artifacts. If model artifacts are modied or deleted, either accidentally or deliberately, version control allows you to roll back to a previous stable release. This can be used in cases where an unauthorized user gains access to the environment and makes changes to the model. If model artifacts are stored in Amazon S3, versioning should be enabled. S3 versioning should also be paired withmulti-factor authentication (MFA) delete, to help ensure that only users authenticated with MFA can permanently delete an object version, or change the versioning state of the bucket. Another way of enabling version control is toassociate Git repositories with new or existing SageMaker notebook instances. SageMaker supportsAWS CodeCommit, GitHub, and other Git-based repositories. Using CodeCommit, repository can be further secured byrotating credentials and enabling MFA.Additionally, the SageMaker Model registry can also be used to register, deploy, and manage models as discussed in SageMaker Pipelines in the MLOps section earlier.Security complianceThird-party auditors assess the security and compliance of Amazon SageMaker as part of multiple AWS compliance programs including FedRAMP, HIPAA, and others. For a list of AWS services in scope of specic compliance programs, see AWS Services in Scope by Compliance Program. Third-party audit reports can be downloaded using AWS Artifact. The customers compliance responsibility when using Amazon SageMaker is determined by the sensitivity of the Organizations data, its compliance objectives, and applicable laws and regulations. AWS provides the following resources to help with compliance:Security and Compliance Quick Start Guides  These deployment guides discuss architectural considerations and provide steps for deploying security- and compliance-focused baseline environments on AWS.Architecting for HIPAA Security and Compliance Whitepaper  This whitepaper describes how organizations can use AWS to help create HIPAA-compliant applications.AWS Compliance Resources  This collection of workbooks and guides might apply to the Organizations industry and location.AWS Cong  This AWS service assesses how well resource congurations comply with internal practices, industry guidelines, and regulations. As an example, AWS Congcan be used to create compliance rules that can scanAWS Key Management Service (AWS KMS)key policies to determine whether these policies align with the principle of granting least privilege to users. Please refer to theHow to use AWS Cong to determine compliance of AWS KMS key policies to your specications, which outlines this process.AWS Security Hub  This AWS service provides a comprehensive view of the security state within AWS that helps check compliance with security industry standards and best practices.Cost optimizationCost management is a primary concern for public sector organizations projects to ensure the best use of public funds while enabling agency missions. AWS provides several mechanisms to manage costs in each phase of the ML lifecycle (Prepare, Build, Train & Tune, Deploy, and Manage) as described in this section.PrepareThis step of the ML lifecycle includes storing the data, labeling the data, and processing the data. Cost control in this phase can be accomplished using the following techniques:Data Storage: ML requires extensive data exploration and transformation. Multiple redundant copies of data are quickly generated, which can lead to exponential growth in storage costs. Therefore, it is essential to establish a cost control strategy at the storage level. Processes can be established to regularly analyze source data and either remove duplicative data or archive data to lower cost storage based on compliance policies. For example, for data stored in S3, S3 storage class analysis can be enabled on any group of objects (based on prex or object tagging) to automatically analyze storage access patterns. This enables identication and transition of rarely-accessed data to S3 glacier, lowering costs. S3 intelligent storage can also be used to lower costs of data that has unpredictable usage patterns. It works by monitoring and moving data between a data tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access.Data Labeling. Data labeling is a key process of identifying raw data (such as images, text les, and videos) and adding one or more meaningful and informative labels to provide context so that an ML model can learn from it. This process can be very time consuming and can quickly increase costs of a project.Amazon SageMaker Ground Truth can be used to reduce these costs. Ground Truths automated data labeling utilizes the Active Learning ML technique to reduce the number of labels required for models, thereby lowering these costs. Ground Truth also provides additional mechanisms such as crowdsourcing with Amazon Mechanical Turk or another vendor company, that can be chosen to lower the costs of labeling.Data Wrangling. In ML, a lot of time is spent in identifying, converting, transforming, and validating raw source data into features that can be used to train models and make predictions. Amazon SageMaker Data Wrangler can be used to reduce this time spent, lowering the costs of the project. With Data Wrangler, data can be imported from various data sources, and transformed without requiring coding. Once data is prepared, fully automated ML workows can be built with Amazon SageMaker Pipelines and saved for reuse in the Amazon SageMaker Feature Store, eliminating the costs incurred in preparing this data again.BuildThis step of the ML lifecycle involves building ML models. Cost control in this phase can be accomplished using the following techniques:Notebook Utilization. AnAmazon SageMaker notebook instanceis a ML compute instance running the Jupyter Notebook. It helps prepare and process data, write code to train models, deploy models to SageMaker hosting, and test or validate models. Costs incurred can be reduced signicantly by optimizing notebook utilization. One way is to stop the notebook instance when its not being used and starting it up only when needed. Another option is to use alifecycle conguration script that automatically shuts down the instance when not being worked on. (SeeRight-sizing resources and avoiding unnecessary costs in Amazon SageMaker for details.)Test code locally. The SageMaker Python SDK supports local mode, which allows creation of estimators and deployment to the local environment. Before a training job is submitted, running thetfunction in local mode enables early feedback prior to running in SageMakers managed training or hosting environments. Issues with code and data can be resolved early to reduce costs incurred in failed training jobs. This also saves time spent in initializing the training cluster.Use Pipe mode (where applicable) to reduce training time. Certain algorithms in Amazon SageMaker, such as Blazing text, work on a large corpus of data. When these jobs are launched, signicant time goes into downloading the data fromAmazon S3 into Amazon EBS. Training jobs dont start until this download nishes.These algorithms can take advantage ofPipe mode,in which training data is streamed from Amazon S3 into Amazon EBS to start training jobs immediately.Find the right balance: Performance vs. accuracy. 32-bit (single precision or FP32) and even 64-bit (double precision or FP64) oating point variables are popular for many applications that require high precision. These are workloads such as engineering simulations that simulate real-world behavior and need the mathematical model to be as exact as possible. In many cases, however, moving to half or mixed precision (16-bit or FP16) reduces training time and consequently costs less, and is worth the minor tradeo.s in accuracy. Seethis Accelerating GPU computation through mixed-precision methodsfor details. A similar trade-o. also applies when deciding on the number of layers in a neural network for classication algorithms, such as image classication. Throughput of 16-bit oating point and 32-bit oating point calculations need to be compared to determine an appropriate approach for the model in question.Jumpstart: Developers who are new to ML often learn that importing an ML model from a third-party source and getting an API endpoint up and running to deploy the model can be time-consuming. The end-to-end process of building a solution, including building, training, and deploying a model, and assembling di.erent components, can take months for users new to ML. SageMaker JumpStart accelerates time-to-deploy over 150 open-source models and provides pre-built solutions, precongured with all necessary AWS services required to launch the solution into production, including CloudFormation templates and reference architecture.AWS Marketplace: AWS Marketplace is a digital catalog with listings from independent software vendors to nd, test, buy, and deploy software that runs on AWS. AWS Marketplace provides many pre-trained, deployable ML models for SageMaker. Pre-training the models enables the delivery of ML-powered features faster and at a lower cost.Train and TuneThis step of the ML lifecycle involves providing the algorithm selected in the build phase with the training data to learn from, and setting the model parameters to optimize the training process. Cost control in this phase can be accomplished using the following techniques:Use Spot Instances. If the training job can be interrupted, Amazon SageMaker Managed spot training can be used to optimize the cost of training models up to 90% over On-Demand Instances. Training jobs can be congured to use Spot Instances and a stopping condition can be used to specify how long Amazon SageMaker waits for a job to run using EC2 Spot Instances. Seethis Managed Spot Training: Save Up to 90% On Your Amazon SageMaker Training Jobs for details.Hyperparameter optimization (HPO). Amazon SageMakers built-in HPO automatically adjusts hundreds of di.erent combinations of parameters to quickly arrive at the best solution for your ML problem. When combined with high-performance algorithms, distributed computing, and managed infrastructure, built-in HPO drastically decreases the training time and overall cost of building production-grade systems. Built-in HPO works best with a reduced search space.CPU vs GPU. CPUs are best at handling single, more complex calculations sequentially, whereas GPUs are better at handling multiple but simple calculations in parallel. GPUs provide a great price/performance ratio if e.ectively used. However, GPUs also cost more, and should be chosen only when really needed. For many use cases, a standard current generation instance type from an instance family such as ml.m* provides enough computing power, memory, and network performance for many Jupyter notebooks to perform well. A best practice is to start with the minimum requirement in terms of ML instance specication and work up to identifying the best instance type and family for the model in question.Distributed Training. When using massive datasets for training, the process can be sped up by distributing training on multiple machines or processes in a cluster as described earlier. Another option is to use a small subset of data for development, and use the full dataset for a training job that is distributed across optimized instances such as P2 or P3 GPU instances or an instance with powerful CPU, such as c5.Monitor the performance of your training jobs to identify waste. Amazon SageMaker is integrated with CloudWatch out of the box and publishes instance metrics of the training cluster in CloudWatch. These metrics enable adjustments to the cluster, such as CPUs, memory, number of instances, and more. Also, Amazon SageMaker Debugger provides full visibility into model training by monitoring, recording, analyzing, and visualizing training process tensors. Debugger can reduce the time, resources, and cost needed to train models.Deploy and ManageThis step of the ML lifecycle involves deployment of the model to get predictions, and managing the model to ensure it meets functional and non-functional requirements of the application. Cost control in this phase can be accomplished using the following techniques:Endpoint deployment: Amazon SageMaker enables testing of new models using A/B testing. Endpoints need to be deleted when testing is completed to reduce costs. These can be recreated from S3 if and when needed. Endpoints that are not deleted can be automatically detected by using EventBridge / CloudWatch Events and Lambda functions. For example, you can detect if endpoints have been idle (with no invocations over a certain period, such as 24 hours), and send an email or text message with the list of detected idle endpoints using SNS. See this Right-sizing resources and avoiding unnecessary costs in Amazon SageMaker for details.Multi-model endpoints. SageMaker endpoints provide the capability to host multiple models.Multi-model endpointsreduce hosting costs by improving endpoint utilization, and provide a scalable and cost-e.ective solution to deploying a large number of models. Multi-model endpoints enable time-sharing of memory resources across models. It also reduces deployment overhead because Amazon SageMaker loads models in memory and scales them based on tra.c patterns.Auto Scaling. Amazon SageMaker Auto Scaling optimizes the cost of model endpoints. Auto Scaling automatically increases the number of instances to handle increase in load (scale out) and decreases the number of instances when not needed (scale in), thereby reducing operational costs. The endpoint can be monitored to adjust the scaling policy based on the CloudWatch metrics. (SeeLoad test and optimize an Amazon SageMaker endpoint using automatic scaling for details).Amazon Elastic Inference for deep learning. For inferences, a deep learning application may not fully utilize the capacity o.ered by a GPU. UsingAmazon Elastic Inference allows the attachment of low-cost GPU-powered acceleration to Amazon EC2 and Amazon SageMaker instances to reduce the cost of running deep learning inference by up to 75%.Analyzing costs with Cost Explorer. Cost Explorer is a tool that enables viewing and analyzing AWS service-related costs and usage including SageMaker. Cost allocation tags can be used to get views of costs aggregated across specic views, such as a project. To accomplish this, all Amazon SageMaker project-related resources, including notebook instances and the hosting endpoint, can be tagged with user-dened tags. For example, tags can be the name of the project, business unit, or environment (such as development, testing, or production). After user-dened tags have been dened and created, they will need to be activated in the Billing and Cost Management console for cost allocation tracking. These tags can then be used to get di.erent views of costs using Cost Explorer as well as Cost and Usage Reports (Cost Allocation Tags appear on the console after Cost Explorer, Budgets, and AWS Cost and Usage Reports have been enabled).AWS Budgets. AWS Budgets help you manage Amazon SageMaker costs, including development, training, and hosting, by setting alerts and notications when cost or usage exceeds (or is forecasted to exceed) the budgeted amount. After a budget is created, progress can be tracked on the AWS Budgets console. Service Catalog can be integrated with AWS Budgets to create and associate budgets with portfolios and products, and keep developers informed on resource costs for running cost-aware workloads. See Cost Control Blog Series #2: Automate Cost Control using Service Catalog and AWS Budgets for details.Bias and ExplainabilityDemonstrating explainability is a signicant challenge because complex ML models are hard to understand and even harder to interpret and debug. There is an inherent tension between ML performance (predictive accuracy) and explainability; often the highest performing methods are the least explainable, and the most explainable are less accurate. Hence, public sector organizations need to invest signicant time with appropriate tools, techniques, and mechanisms to demonstrate explainability and lack of bias in their ML models, which could be a deterrent to adoption.AWS Cloud provides the following capabilities and services to assist public sector organizations in resolving these challenges.Amazon SageMaker DebuggerAmazon SageMaker Debugger provides visibility into the model training process for real-time and o.ine analysis. In the existing training code for TensorFlow, Keras, Apache MXNet, PyTorch, and XGBoost, the newSageMaker DebuggerSDK can be used to save the internal model state at periodic intervals in S3. This state is composed of a number of components: The parameters being learned by the model (for example, weights and biases for neural networks), the changes applied to these parameters by the optimizer (gradients), optimization parameters, scalar values such as accuracies and losses, and outputs of each layer of a neural network.SageMaker Debugger provides three built-in tensor collections called feature importance,  average_shap, and full_shap, to visualize and analyze captured tensors specifically for  model explanation. Feature importance is a technique that explains the features that make  up the training data using a score (importance). It indicates how useful or valuable the  feature is, relative to other features.SHAP (SHapley Additive exPlanations) is an open-source technique based on coalitional game  theory. It explains an ML prediction by assuming that each feature value of training  data instance is a player in a game in which the prediction is the payout. Shapley values  indicate how to distribute the payout fairly among the features. The values consider all  possible predictions for an instance and use all possible combinations of inputs. Because  of this exhaustive approach, SHAP can guarantee consistency and local accuracy. For more  information, see the SHAP website.SHAP values can be used for global explanatory methods to understand the model and its feature contributions in aggregate over multiple data points.SHAP values can also be used for local explanations that focus on explaining each individual prediction. See ML Explainability with Amazon SageMaker Debugger for details.Amazon SageMaker ClarifyAmazon SageMaker Clarify is a service that is integrated into SageMaker Studio and detects potential bias during data preparation, model training, and in deployed models, by examining specied attributes. For instance, bias in attributes related to age can be examined in the initial dataset, in the trained as well as the deployed model, and quantied in a detailed report. Clarify provides a range of metrics to measure bias such as Di.erence in positive proportions in labels (DPL), Di.erence in positive proportions in predicted labels (DPPL), Accuracy di.erence (AD), and Counterfactuals  Fliptest (FT). In addition, SageMaker Clarify also enables explainability by including feature importance graphs using SHAP to help explain model predictions. It produces reports and visualizations that can be used to support internal presentations on a models predictions. See New  Amazon SageMaker Clarify Detects Bias and Increases the Transparency of Machine Learning Models for details. Clarify has been designed to work without burdening the inference operations  assessment of a model can be spun o. as a separate activity in SageMaker.This capability is very helpful to automate monitoring drift.SHAP and LIME (Local Interpretable Model-Agnostic Explanations) libraries:In case team members are unable to use Amazon SageMaker Debugger or Amazon SageMaker Clarify for explainability and bias, their libraries can directly be installed on SageMaker Jupyter instances or Studio Notebooks and incorporated into the training code See Explaining Amazon SageMaker Autopilot models with SHAP for details on using SHAP. LIME provides a model-agnostic approach for setting up explanations; LIME builds sparse linear models around each prediction to explain how the black box model works in that local vicinity. SHAP is a more cost-intensive process as it requires more compute time calculating all the probable combinations and permutations of features for explaining predictions compared to LIME. 4 567891011121314151617181920212223 Machine Learning Best Practices for Public Sector Organizations AWS WhitepaperConclusionUS Public sector organizations have complex mission objectives and are increasingly adopting ML services to help with their initiatives. ML can transform the way government agencies operate, and enable them to provide improved citizen services. However, several barriers remain for these organizations to implement ML. This whitepaper outlined some of the challenges and provided best practices that can help address these challenges using AWS Cloud. 24 Machine Learning Best Practices for Public Sector Organizations AWS WhitepaperNext StepsAdopting the AWS Cloud can provide you with sustainable advantages for telehealth systems. Your AWS account team can work together with your team and/or your chosen member of the AWS Partner Network (APN) to implement your enterprise cloud computing initiatives. You can reach out to an AWS partner through the AWS Partner Network. Get started on AI and ML by visiting AWS ML, AWS ML Embark Program, or the ML Solutions Lab. 25 Machine Learning Best Practices for Public Sector Organizations AWS WhitepaperReferences to Public Sector Use CasesThe following list provides some examples of public sector use cases for AI/ML in AWS. For a more comprehensive list, refer to the AWS Blog.https://www.amazon.science/how-nasa-uses-aws-to-protect-life-and-infrastructure-on-earthhttps://www.amazon.science/blog/paper-on-forecasting-spread-of-covid-19-wins-best-paper-awardhttps://www.amazon.science/blog/amazon-supports-nsf-research-in-human-ai-interaction-collaborationhttps://aws.amazon.com/blogs/machine-learning/ne-tune-and-deploy-the-protbert-model-for-protein-classication-using-amazon-SageMaker/https://aws.amazon.com/blogs/publicsector/using-ai-rethink-document-automation-extract-insights/https://aws.amazon.com/blogs/publicsector/chestereld-county-public-schools-uses-machine-learning-predict-countys-chronic-absenteeism/https://aws.amazon.com/blogs/publicsector/using-advanced-analytics-accelerate-problem-resolution-public-sector/https://aws.amazon.com/blogs/publicsector/how-ai-and-ml-are-helping-tackle-the-global-teacher-shortage/https://aws.amazon.com/blogs/publicsector/improving-school-safety-how-cloud-helping-k12-students-wake-violence/https://aws.amazon.com/blogs/publicsector/heading-into-hurricane-season/https://aws.amazon.com/blogs/publicsector/helping-to-end-future-famines-with-machine-learning/ 26 Machine Learning Best Practices for Public Sector Organizations AWS Whitepaper
