{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a5ab2f-d044-4956-b75b-7408d9c3e323",
   "metadata": {},
   "source": [
    "# Leveraging Agents with Bedrock\n",
    "\n",
    "> *This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio. You can also run on a local setup, as long as you have the right IAM credentials to invoke the Claude model via Bedrock*\n",
    "\n",
    "---\n",
    "\n",
    "In this demo notebook, we demonstrate an implementation of Function Calling with Anthropic's Claude models via Bedrock. This notebook is inspired by the [original work](https://drive.google.com/drive/folders/1-94Fa3HxEMkxkwKppe8lp_9-IXXvsvv1) by the Anthropic Team and modified it for use with Amazon Bedrock.\n",
    "\n",
    "\n",
    "This notebook need access to anthropic.claude-3-sonnet-20240229-v1:0 model in Bedrock\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeedd9f-f0a3-4f8e-934d-22f6f7a89de5",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Conversational interfaces such as chatbots and virtual assistants can be used to enhance the user experience for your customers. These use natural language processing (NLP) and machine learning algorithms to understand and respond to user queries and can be used in a variety of applications, such as customer service, sales, and e-commerce, to provide quick and efficient responses to users. usuallythey are augmented by fetching information from various channels such as websites, social media platforms, and messaging apps which involve a complex workflow as shown below\n",
    "\n",
    "\n",
    "### Chatbot using Amazon Bedrock\n",
    "\n",
    "![Amazon Bedrock - Agents Interface](./images/agents.jpg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Building  - Key Elements\n",
    "\n",
    "The first process in a building a contextual-aware chatbot is to identify the tools which can be called by the LLM's. \n",
    "\n",
    "Second process is the user request orchestration , interaction,  invoking and returing the results\n",
    "\n",
    "### Architecture [Weather lookup]\n",
    "We Search and look for the Latitude and Longitude and then invoke the weather app to get predictions\n",
    "\n",
    "![Amazon Bedrock - Agents Interface](./images/weather.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5edcc6",
   "metadata": {},
   "source": [
    "#### Please un comment and install the libraries below if you do not have these "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a8c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain==0.1.17\n",
    "#!pip install langchain-anthropic\n",
    "#!pip install boto3==1.34.95\n",
    "#!pip install faiss-cpu==1.8.0\n",
    "#!pip install pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1912ba84",
   "metadata": {},
   "source": [
    "#### To install the langchain-aws\n",
    "\n",
    "you can run the `pip install langchain-aws`\n",
    "\n",
    "to get the latest release use these commands below"
   ]
  },
  {
   "cell_type": "raw",
   "id": "01HWZSMYZ3G0D7BH0F8ZQB3XGH",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#- run them from a terminal on your machine\n",
    "cd ~\n",
    "mkdir temp_t\n",
    "cd temp_t\n",
    "git clone https://github.com/langchain-ai/langchain-aws/\n",
    "pip install ./langchain-aws/libs/aws/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27610c0f-7de6-4440-8f76-decf30e3c5ca",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "⚠️ ⚠️ ⚠️ Before running this notebook, ensure you have the required libraries and access to internet for the weather api's in this notebook. ⚠️ ⚠️ ⚠️\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19cb8580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "\n",
    "def get_bedrock_client(assumed_role: Optional[str] = None, region: Optional[str] = 'us-east-1',runtime: Optional[bool] = True,external_id=None, ep_url=None):\n",
    "    \"\"\"Create a boto3 client for Amazon Bedrock, with optional configuration overrides \n",
    "    \"\"\"\n",
    "    target_region = region\n",
    "\n",
    "    print(f\"Create new client\\n  Using region: {target_region}:external_id={external_id}: \")\n",
    "    session_kwargs = {\"region_name\": target_region}\n",
    "    client_kwargs = {**session_kwargs}\n",
    "\n",
    "    profile_name = os.environ.get(\"AWS_PROFILE\")\n",
    "    if profile_name:\n",
    "        print(f\"  Using profile: {profile_name}\")\n",
    "        session_kwargs[\"profile_name\"] = profile_name\n",
    "\n",
    "    retry_config = Config(\n",
    "        region_name=target_region,\n",
    "        retries={\n",
    "            \"max_attempts\": 10,\n",
    "            \"mode\": \"standard\",\n",
    "        },\n",
    "    )\n",
    "    session = boto3.Session(**session_kwargs)\n",
    "\n",
    "    if assumed_role:\n",
    "        print(f\"  Using role: {assumed_role}\", end='')\n",
    "        sts = session.client(\"sts\")\n",
    "        if external_id:\n",
    "            response = sts.assume_role(\n",
    "                RoleArn=str(assumed_role),\n",
    "                RoleSessionName=\"langchain-llm-1\",\n",
    "                ExternalId=external_id\n",
    "            )\n",
    "        else:\n",
    "            response = sts.assume_role(\n",
    "                RoleArn=str(assumed_role),\n",
    "                RoleSessionName=\"langchain-llm-1\",\n",
    "            )\n",
    "        print(f\"Using role: {assumed_role} ... sts::successful!\")\n",
    "        client_kwargs[\"aws_access_key_id\"] = response[\"Credentials\"][\"AccessKeyId\"]\n",
    "        client_kwargs[\"aws_secret_access_key\"] = response[\"Credentials\"][\"SecretAccessKey\"]\n",
    "        client_kwargs[\"aws_session_token\"] = response[\"Credentials\"][\"SessionToken\"]\n",
    "\n",
    "    if runtime:\n",
    "        service_name='bedrock-runtime'\n",
    "    else:\n",
    "        service_name='bedrock'\n",
    "\n",
    "    if ep_url:\n",
    "        bedrock_client = session.client(service_name=service_name,config=retry_config,endpoint_url = ep_url, **client_kwargs )\n",
    "    else:\n",
    "        bedrock_client = session.client(service_name=service_name,config=retry_config, **client_kwargs )\n",
    "\n",
    "    print(\"boto3 Bedrock client successfully created!\")\n",
    "    print(bedrock_client._endpoint)\n",
    "    return bedrock_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2b2a05-78a9-40ca-9b5e-121030f9ede1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "\n",
    "bedrock_runtime = get_bedrock_client() #\n",
    "#     assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "#     region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb6bee-7654-4269-9127-9afa4e823454",
   "metadata": {},
   "source": [
    "### Anthropic Claude\n",
    "\n",
    "#### Input\n",
    "\n",
    "```json\n",
    "\n",
    "\"messages\": [\n",
    "    {\"role\": \"user\", \"content\": \"Hello, Claude\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you describe LLMs to me?\"}\n",
    "        \n",
    "]\n",
    "{\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 100,\n",
    "    \"messages\": messages,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.9\n",
    "} \n",
    "```\n",
    "\n",
    "#### Output\n",
    "\n",
    "```json\n",
    "{\n",
    "    'id': 'msg_01T',\n",
    "    'type': 'message',\n",
    "    'role': 'assistant',\n",
    "    'content': [\n",
    "        {\n",
    "            'type': 'text',\n",
    "            'text': 'Sure, the concept...'\n",
    "        }\n",
    "    ],\n",
    "    'model': 'model_id',\n",
    "    'stop_reason': 'max_tokens',\n",
    "    'stop_sequence': None,\n",
    "    'usage': {'input_tokens':xy, 'output_tokens': yz}}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c0fe6-576a-4380-89aa-726bab5d65ff",
   "metadata": {},
   "source": [
    "### Bedrock model\n",
    "\n",
    "Anthropic Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeff818",
   "metadata": {},
   "source": [
    "The key for this to work is to let LLM which is Claude models know about a set of `tools` that it has available i.e. functions it can call between a set of tags. This is possible because Anthropic's Claude models have been extensively trained on such tags in its training corpus.\n",
    "\n",
    "Then present a way to call the tools in a step by step fashion till it gets the right answer. We create a set of callable functions , below e present a sample functions which can be modified to suit your needs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee67162",
   "metadata": {},
   "source": [
    "#### Helper function to pretty print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b8a161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from typing import Optional, List, Any\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8178d7fd",
   "metadata": {},
   "source": [
    "## Section 1. Connectivity and invocation\n",
    "\n",
    "**Invoke the model to ensure connectivity** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a36ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" #\"anthropic.claude-v2\"\n",
    "\n",
    "messages=[\n",
    "    { \n",
    "        \"role\":'user', \n",
    "        \"content\":[{\n",
    "            'type':'text',\n",
    "            'text': \"What is quantum mechanics? \"\n",
    "        }]\n",
    "    },\n",
    "    { \n",
    "        \"role\":'assistant', \n",
    "        \"content\":[{\n",
    "            'type':'text',\n",
    "            'text': \"It is a branch of physics that describes how matter and energy interact with discrete energy values \"\n",
    "        }]\n",
    "    },\n",
    "    { \n",
    "        \"role\":'user', \n",
    "        \"content\":[{\n",
    "            'type':'text',\n",
    "            'text': \"Can you explain a bit more about discrete energies?\"\n",
    "        }]\n",
    "    }\n",
    "]\n",
    "body=json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 100,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0.5,\n",
    "            \"top_p\": 0.9\n",
    "        }  \n",
    "    )  \n",
    "    \n",
    "response = bedrock_runtime.invoke_model(body=body, modelId=modelId)\n",
    "response_body = json.loads(response.get('body').read())\n",
    "print_ww(response_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc2e41",
   "metadata": {},
   "source": [
    "## Section 2 -- Tooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb60ff51",
   "metadata": {},
   "source": [
    "###  We will connect a Vector DB and expose that as a tool \n",
    "\n",
    "Create a set of helper function\n",
    "\n",
    "we will create a set of functions which we can the re use in our application\n",
    "1. We will need to create a prompt template. This template helps Bedrock models understand the tools and how to invoke them.\n",
    "2. Create a method to read the available tools and add it to the prompt being used to invoke Claude\n",
    "3. Call function which will be responsbile to actually invoke the function with the `right` parameters\n",
    "4. Format Results for helping the Model leverage the results for summarization\n",
    "5. `Add to prompt`. The result which come back need to be added to the the prompt and model invoked again to get the right results\n",
    "\n",
    "[See this notebook for more details](https://github.com/aws-samples/amazon-bedrock-samples/blob/main/rag-solutions/rag-foundations-workshop/notebooks/05_agent_based_text_generation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73fdb2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8103b5",
   "metadata": {},
   "source": [
    "### Add Tools\n",
    "\n",
    "Recursively add the available tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0d72b4",
   "metadata": {},
   "source": [
    "### With LangChain\n",
    "\n",
    "Now use langchain and annotations to create the tools and invoke the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f64328fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from langchain.tools import tool\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain import LLMMathChain\n",
    "\n",
    "@tool (\"get_lat_long\")\n",
    "def get_lat_long(place: str) -> dict:\n",
    "    \"\"\"Returns the latitude and longitude for a given place name as a dict object of python.\"\"\"\n",
    "    url = \"https://nominatim.openstreetmap.org/search\"\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    params = {'q': place, 'format': 'json', 'limit': 1}\n",
    "    response = requests.get(url, params=params, headers=headers).json()\n",
    "\n",
    "    if response:\n",
    "        lat = response[0][\"lat\"]\n",
    "        lon = response[0][\"lon\"]\n",
    "        return {\"latitude\": lat, \"longitude\": lon}\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "@tool (\"get_weather\")\n",
    "def get_weather(latitude: str, longitude: str) -> dict:\n",
    "  \"\"\"Returns weather data for a given latitude and longitude.\"\"\"\n",
    "  url = f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current_weather=true\"\n",
    "  response = requests.get(url)\n",
    "  print_ww(f\"get_weather:tool:invoked::response={response}:\")\n",
    "  return response.json()\n",
    "\n",
    "#get_weather_tool = StructuredTool.from_function(get_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "445b9ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_list = [get_lat_long,get_weather]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc6179",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tools_s in tools_list:\n",
    "    print_ww(f\"Tool:name={tools_s.name}::args={tools_s.args}:: discription={tools_s.description}::\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3301e5",
   "metadata": {},
   "source": [
    "### Tooling and Agents\n",
    "\n",
    "**Use the Default prompt template**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01e922b",
   "metadata": {},
   "source": [
    "#### Add the retriever tooling\n",
    "\n",
    "**Use In-Memory FAISS DB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain import LLMMathChain\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate,HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" #\"anthropic.claude-v2\"\n",
    "react_agent_llm = BedrockChat(\n",
    "    model_id=modelId,\n",
    "    client=bedrock_runtime,\n",
    "    model_kwargs={\"temperature\": 0.1},\n",
    ")\n",
    "\n",
    "tools_list = [get_lat_long,get_weather]\n",
    "\n",
    "\n",
    "react_agent = initialize_agent(\n",
    "    tools_list, \n",
    "    react_agent_llm, \n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, #SELF_ASK_WITH_SEARCH,# CONVERSATIONAL_REACT_DESCRIPTION, # ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True,\n",
    "    max_iteration=4,\n",
    "    return_intermediate_steps=True,\n",
    "    max_execution_time=60,\n",
    "    #    handle_parsing_errors=True,\n",
    "    #prompt = prompt_template\n",
    ")\n",
    "print_ww(f\"The prompt:template:used:was ---- > \\n\\n{react_agent.agent.llm_chain.prompt}\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8796092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.embeddings.bedrock import BedrockEmbeddings\n",
    "\n",
    "loader = PyPDFLoader(\"./rag_data/Amazon_SageMaker_FAQs.pdf\")\n",
    "bedrock_client = get_bedrock_client()\n",
    "texts = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0).split_documents(loader.load())\n",
    "embed_model = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=bedrock_client)\n",
    "#- create the vector store\n",
    "db = FAISS.from_documents(texts, embed_model)\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "tool_search = create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name=\"search_sagemaker_policy\",\n",
    "    description=\"Searches and returns excerpts for any question about SageMaker\",\n",
    ")\n",
    "print_ww(tool_search.func)\n",
    "tool_search.args_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbac21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "tools_list = [get_lat_long,get_weather]\n",
    "\n",
    "\n",
    "tools_list.append(tool_search)\n",
    "\n",
    "# Construct the Tools agent and also the executor - you do not need the executor class\n",
    "react_agent = initialize_agent(\n",
    "    tools_list, \n",
    "    react_agent_llm, \n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, #SELF_ASK_WITH_SEARCH,# CONVERSATIONAL_REACT_DESCRIPTION, # ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True,\n",
    "    max_iteration=4,\n",
    "    return_intermediate_steps=True,\n",
    "    #handle_parsing_errors=True,\n",
    "    #prompt = chat_prompt_template,\n",
    ")\n",
    "\n",
    "\n",
    "react_agent.invoke({\"input\": \"can you check the weather in Marysville WA for me?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec23f7",
   "metadata": {},
   "source": [
    "#### Now invoke from FAQ's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138ff91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "react_agent.invoke({\"input\": \"What is Amazon SageMaker Clarify?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed6fc3e",
   "metadata": {},
   "source": [
    "## Section 2 Use the Langchain-AWS classes \n",
    "These classes having all thr latest api's and working correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6307a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" #\"anthropic.claude-v2\"\n",
    "chat = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    model_kwargs={\"temperature\": 0.1},\n",
    "    client=bedrock_runtime\n",
    ")\n",
    "\n",
    "import requests\n",
    "\n",
    "from langchain.tools import tool\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain import LLMMathChain\n",
    "\n",
    "@tool (\"get_lat_long\")\n",
    "def get_lat_long(place: str) -> dict:\n",
    "    \"\"\"Returns the latitude and longitude for a given place name as a dict object of python.\"\"\"\n",
    "    url = \"https://nominatim.openstreetmap.org/search\"\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    params = {'q': place, 'format': 'json', 'limit': 1}\n",
    "    response = requests.get(url, params=params,headers=headers).json()\n",
    "\n",
    "    if response:\n",
    "        lat = response[0][\"lat\"]\n",
    "        lon = response[0][\"lon\"]\n",
    "        return {\"latitude\": lat, \"longitude\": lon}\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "@tool (\"get_weather\")\n",
    "def get_weather(latitude: str, longitude: str) -> dict:\n",
    "  \"\"\"Returns weather data for a given latitude and longitude.\"\"\"\n",
    "  url = f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current_weather=true\"\n",
    "  response = requests.get(url)\n",
    "  print_ww(f\"get_weather:tool:invoked::response={response}:\")\n",
    "  return response.json()\n",
    "\n",
    "#get_weather_tool = StructuredTool.from_function(get_weather)\n",
    "\n",
    "\n",
    "llm_with_tools = chat.bind_tools([get_weather,get_lat_long])\n",
    "print_ww(llm_with_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebde4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.human import HumanMessage\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"what is the weather like in San Francisco\"\n",
    "    )\n",
    "]\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588db206",
   "metadata": {},
   "source": [
    "### Use the ChatBedrock class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeacee7",
   "metadata": {},
   "source": [
    "#### Add Retriever Tool with functions\n",
    "\n",
    "**Add the retriever tool along with the other function calls**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092729fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock # required until llama_index offers direct Bedrock integration\n",
    "from langchain.embeddings.bedrock import BedrockEmbeddings\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(\"./rag_data/Amazon_SageMaker_FAQs.pdf\")\n",
    "bedrock_client = get_bedrock_client()\n",
    "texts = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0).split_documents(loader.load())\n",
    "embed_model = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=bedrock_client)\n",
    "#- create the vector store\n",
    "db = FAISS.from_documents(texts, embed_model)\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b88dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "tool_search = create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name=\"search_sagemaker_policy\",\n",
    "    description=\"Searches and returns excerpts for any question about SageMaker\",\n",
    ")\n",
    "print_ww(tool_search.func)\n",
    "tool_search.args_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e97585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "tools_list = [get_lat_long,get_weather]\n",
    "\n",
    "\n",
    "tools_list.append(tool_search)\n",
    "\n",
    "react_agent_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=bedrock_runtime,\n",
    "    #model_kwargs={\"max_tokens_to_sample\": 100},\n",
    "    model_kwargs={\"temperature\": 0.1},\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeebf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "prompt_template_sys = \"\"\"\n",
    "\n",
    "Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do, Also try to follow steps mentioned above\n",
    "Action: the action to take, should be one of [ \"get_lat_long\", \"get_weather\", \"Calculator\"]\n",
    "Action Input: the input to the action\\nObservation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Assistant:\n",
    "{agent_scratchpad}'\n",
    "\n",
    "\"\"\"\n",
    "messages=[\n",
    "    SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'input'], template=prompt_template_sys)), \n",
    "    HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))\n",
    "]\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "print_ww(f\"from:messages:prompt:template:{chat_prompt_template}\")\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate(\n",
    "    input_variables=['agent_scratchpad', 'input'], \n",
    "    messages=messages\n",
    ")\n",
    "print_ww(f\"Crafted::prompt:template:{chat_prompt_template}\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#react_agent_llm.bind_tools = custom_bind_func\n",
    "\n",
    "# Construct the Tools agent\n",
    "react_agent = create_tool_calling_agent(react_agent_llm, tools_list,chat_prompt_template)\n",
    "agent_executor = AgentExecutor(agent=react_agent, tools=tools_list, verbose=True, max_iterations=5, return_intermediate_steps=True)\n",
    "agent_executor.invoke({\"input\": \"can you check the weather in Marysville WA for me?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1862ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"What is Amazon SageMaker Clarify?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffac26f",
   "metadata": {},
   "source": [
    "## Section 3 Chat tools with history\n",
    "\n",
    "**Use the Agent Executor chain with History for chat** \n",
    "\n",
    "here we are simulating as if the model had responded with the first question which was `what is clarify` and we have a follow on question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e30b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What kind of bias does it detect?\",\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"hi! my name is bob\"),\n",
    "            AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "            HumanMessage(content=\"What is Amazon SageMaker Clarify?\"),\n",
    "            AIMessage(content=\"Amazon SageMaker Clarify helps detect bias in machine learning models and explain model predictions.\")\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a71d324",
   "metadata": {},
   "source": [
    "#### Creating the chain of Agents + tools manually\n",
    "\n",
    "If you want to create the Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b40bc910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnablePassthrough\n",
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "from langchain.agents.format_scratchpad.tools import (\n",
    "    format_to_tool_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.tools import ToolsAgentOutputParser\n",
    "\n",
    "llm_with_tools = react_agent_llm.bind_tools(tools_list)\n",
    "\n",
    "conversational_agent = (\n",
    "        RunnablePassthrough.assign(\n",
    "            agent_scratchpad=lambda x: format_to_tool_messages(x[\"intermediate_steps\"])\n",
    "        )\n",
    "        | chat_prompt_template\n",
    "        | llm_with_tools\n",
    "        | ToolsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c88ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_t = conversational_agent.invoke({\"input\": \"What is Amazon SageMaker Clarify?\", \"intermediate_steps\": []})\n",
    "\n",
    "print_ww(output_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48a0e8-147d-4525-a6b2-68a09af1b2c4",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "In this notebook we showed some basic examples of leveraging tools and agents when invoking Amazon Bedrock models using the AWS Python SDK. You're now ready to explore the other labs to dive deeper on different use-cases and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5733f3e8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "trainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
