{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a5ab2f-d044-4956-b75b-7408d9c3e323",
   "metadata": {},
   "source": [
    "# Leveraging Agents with Bedrock\n",
    "\n",
    "> *This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio. You can also run on a local setup, as long as you have the right IAM credentials to invoke the Claude model via Bedrock*\n",
    "\n",
    "---\n",
    "\n",
    "In this demo notebook, we demonstrate an implementation of Function Calling with Anthropic's Claude models via Bedrock. This notebook is inspired by the [original work](https://drive.google.com/drive/folders/1-94Fa3HxEMkxkwKppe8lp_9-IXXvsvv1) by the Anthropic Team and modified it for use with Amazon Bedrock.\n",
    "\n",
    "This notebook need access to anthropic.claude-3-sonnet-20240229-v1:0 model in Bedrock\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeedd9f-f0a3-4f8e-934d-22f6f7a89de5",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Conversational interfaces such as chatbots and virtual assistants can be used to enhance the user experience for your customers. These use natural language processing (NLP) and machine learning algorithms to understand and respond to user queries and can be used in a variety of applications, such as customer service, sales, and e-commerce, to provide quick and efficient responses to users. usuallythey are augmented by fetching information from various channels such as websites, social media platforms, and messaging apps which involve a complex workflow as shown below\n",
    "\n",
    "\n",
    "### Chatbot using Amazon Bedrock\n",
    "\n",
    "![Amazon Bedrock - Agents Interface](./images/agents.jpg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Building  - Key Elements\n",
    "\n",
    "The first process in a building a contextual-aware chatbot is to identify the tools which can be called by the LLM's. \n",
    "\n",
    "Second process is the user request orchestration , interaction,  invoking and returing the results\n",
    "\n",
    "### Architecture [Weather lookup]\n",
    "We Search and look for the Latitude and Longitude and then invoke the weather app to get predictions\n",
    "\n",
    "![Amazon Bedrock - Agents Interface](./images/weather.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e72d7f2",
   "metadata": {},
   "source": [
    "#### Please un comment and install the libraries below if you do not have these "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a8c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.1.17\n",
    "!pip install langchain-anthropic\n",
    "!pip install boto3==1.34.95\n",
    "!pip install faiss-cpu==1.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e65a69",
   "metadata": {},
   "source": [
    "#### To install the langchain-aws\n",
    "\n",
    "you can run the `pip install langchain-aws`\n",
    "\n",
    "to get the latest release use these commands below"
   ]
  },
  {
   "cell_type": "raw",
   "id": "01HWZSF4T6VE7SG3C5KJTBVS9S",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#- run them from a terminal on your machine\n",
    "cd ~\n",
    "mkdir temp_t\n",
    "cd temp_t\n",
    "git clone https://github.com/langchain-ai/langchain-aws/\n",
    "pip install ./langchain-aws/libs/aws/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27610c0f-7de6-4440-8f76-decf30e3c5ca",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "⚠️ ⚠️ ⚠️ Before running this notebook, ensure you have the required libraries and access to internet for the weather api's in this notebook. ⚠️ ⚠️ ⚠️\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19cb8580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "\n",
    "def get_bedrock_client(assumed_role: Optional[str] = None, region: Optional[str] = 'us-east-1',runtime: Optional[bool] = True,external_id=None, ep_url=None):\n",
    "    \"\"\"Create a boto3 client for Amazon Bedrock, with optional configuration overrides \n",
    "    \"\"\"\n",
    "    target_region = region\n",
    "\n",
    "    print(f\"Create new client\\n  Using region: {target_region}:external_id={external_id}: \")\n",
    "    session_kwargs = {\"region_name\": target_region}\n",
    "    client_kwargs = {**session_kwargs}\n",
    "\n",
    "    profile_name = os.environ.get(\"AWS_PROFILE\")\n",
    "    if profile_name:\n",
    "        print(f\"  Using profile: {profile_name}\")\n",
    "        session_kwargs[\"profile_name\"] = profile_name\n",
    "\n",
    "    retry_config = Config(\n",
    "        region_name=target_region,\n",
    "        retries={\n",
    "            \"max_attempts\": 10,\n",
    "            \"mode\": \"standard\",\n",
    "        },\n",
    "    )\n",
    "    session = boto3.Session(**session_kwargs)\n",
    "\n",
    "    if assumed_role:\n",
    "        print(f\"  Using role: {assumed_role}\", end='')\n",
    "        sts = session.client(\"sts\")\n",
    "        if external_id:\n",
    "            response = sts.assume_role(\n",
    "                RoleArn=str(assumed_role),\n",
    "                RoleSessionName=\"langchain-llm-1\",\n",
    "                ExternalId=external_id\n",
    "            )\n",
    "        else:\n",
    "            response = sts.assume_role(\n",
    "                RoleArn=str(assumed_role),\n",
    "                RoleSessionName=\"langchain-llm-1\",\n",
    "            )\n",
    "        print(f\"Using role: {assumed_role} ... sts::successful!\")\n",
    "        client_kwargs[\"aws_access_key_id\"] = response[\"Credentials\"][\"AccessKeyId\"]\n",
    "        client_kwargs[\"aws_secret_access_key\"] = response[\"Credentials\"][\"SecretAccessKey\"]\n",
    "        client_kwargs[\"aws_session_token\"] = response[\"Credentials\"][\"SessionToken\"]\n",
    "\n",
    "    if runtime:\n",
    "        service_name='bedrock-runtime'\n",
    "    else:\n",
    "        service_name='bedrock'\n",
    "\n",
    "    if ep_url:\n",
    "        bedrock_client = session.client(service_name=service_name,config=retry_config,endpoint_url = ep_url, **client_kwargs )\n",
    "    else:\n",
    "        bedrock_client = session.client(service_name=service_name,config=retry_config, **client_kwargs )\n",
    "\n",
    "    print(\"boto3 Bedrock client successfully created!\")\n",
    "    print(bedrock_client._endpoint)\n",
    "    return bedrock_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2b2a05-78a9-40ca-9b5e-121030f9ede1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "\n",
    "bedrock_runtime = get_bedrock_client() #\n",
    "#     assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "#     region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb6bee-7654-4269-9127-9afa4e823454",
   "metadata": {},
   "source": [
    "### Anthropic Claude\n",
    "\n",
    "#### Input\n",
    "\n",
    "```json\n",
    "\n",
    "\"messages\": [\n",
    "    {\"role\": \"user\", \"content\": \"Hello, Claude\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you describe LLMs to me?\"}\n",
    "        \n",
    "]\n",
    "{\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 100,\n",
    "    \"messages\": messages,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.9\n",
    "} \n",
    "```\n",
    "\n",
    "#### Output\n",
    "\n",
    "```json\n",
    "{\n",
    "    'id': 'msg_01T',\n",
    "    'type': 'message',\n",
    "    'role': 'assistant',\n",
    "    'content': [\n",
    "        {\n",
    "            'type': 'text',\n",
    "            'text': 'Sure, the concept...'\n",
    "        }\n",
    "    ],\n",
    "    'model': 'model_id',\n",
    "    'stop_reason': 'max_tokens',\n",
    "    'stop_sequence': None,\n",
    "    'usage': {'input_tokens':xy, 'output_tokens': yz}}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c0fe6-576a-4380-89aa-726bab5d65ff",
   "metadata": {},
   "source": [
    "### Bedrock model\n",
    "\n",
    "Anthropic Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeff818",
   "metadata": {},
   "source": [
    "The key for this to work is to let LLM which is Claude models know about a set of `tools` that it has available i.e. functions it can call between a set of tags. This is possible because Anthropic's Claude models have been extensively trained on such tags in its training corpus.\n",
    "\n",
    "Then present a way to call the tools in a step by step fashion till it gets the right answer. We create a set of callable functions , below e present a sample functions which can be modified to suit your needs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee67162",
   "metadata": {},
   "source": [
    "#### Helper function to pretty print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b8a161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from typing import Optional, List, Any\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8178d7fd",
   "metadata": {},
   "source": [
    "## Section 1. Connectivity and invocation\n",
    "\n",
    "**Invoke the model to ensure connectivity** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a36ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" #\"anthropic.claude-v2\"\n",
    "\n",
    "messages=[\n",
    "    { \n",
    "        \"role\":'user', \n",
    "        \"content\":[{\n",
    "            'type':'text',\n",
    "            'text': \"What is quantum mechanics? \"\n",
    "        }]\n",
    "    },\n",
    "    { \n",
    "        \"role\":'assistant', \n",
    "        \"content\":[{\n",
    "            'type':'text',\n",
    "            'text': \"It is a branch of physics that describes how matter and energy interact with discrete energy values \"\n",
    "        }]\n",
    "    },\n",
    "    { \n",
    "        \"role\":'user', \n",
    "        \"content\":[{\n",
    "            'type':'text',\n",
    "            'text': \"Can you explain a bit more about discrete energies?\"\n",
    "        }]\n",
    "    }\n",
    "]\n",
    "body=json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 100,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0.5,\n",
    "            \"top_p\": 0.9\n",
    "        }  \n",
    "    )  \n",
    "    \n",
    "response = bedrock_runtime.invoke_model(body=body, modelId=modelId)\n",
    "response_body = json.loads(response.get('body').read())\n",
    "print_ww(response_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ea167",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_sample_claude_invoke(prompt_str,boto3_bedrock ):\n",
    "    modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" #\"anthropic.claude-v2\"\n",
    "    messages=[{ \n",
    "        \"role\":'user', \n",
    "        \"content\":[{\n",
    "            'type':'text',\n",
    "            'text': prompt_str\n",
    "        }]\n",
    "    }]\n",
    "    body=json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 100,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0.5,\n",
    "            \"top_p\": 0.9\n",
    "        }  \n",
    "    )  \n",
    "    response = boto3_bedrock.invoke_model(body=body, modelId=modelId)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    return response_body\n",
    "\n",
    "\n",
    "test_sample_claude_invoke(\"what is quantum mechanics\", bedrock_runtime)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4f397f",
   "metadata": {},
   "source": [
    "### Invoke Langchain with `Chain` using BedrockChat class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b122500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm_chat = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\", \n",
    "    model_kwargs={\"temperature\": 0.1},\n",
    "    #region_name=\"us-west-2\",\n",
    "    client=bedrock_runtime,\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm_chat, verbose=True, memory=memory\n",
    ")\n",
    "\n",
    "resp = conversation.predict(input=\"Hi there!\")\n",
    "print_ww(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b026656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "#from langchain_openai import ChatOpenAI\n",
    "\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" #\"anthropic.claude-v2\"\n",
    "cl_llm = BedrockChat(\n",
    "    model_id=modelId,\n",
    "    client=bedrock_runtime,\n",
    "    #model_kwargs={\"max_tokens_to_sample\": 100},\n",
    "    model_kwargs={\"temperature\": 0.1},\n",
    ")\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain( llm=cl_llm, verbose=True, memory=memory)\n",
    "\n",
    "\n",
    "prompt_t = ChatPromptTemplate.from_messages([(\"human\", \"Explain this  {question}.\")])\n",
    "chain_t = prompt_t | cl_llm | StrOutputParser()\n",
    "\n",
    "resp = chain_t.invoke({'question':\"what is quantum physics\"}) # has to match the variables\n",
    "print_ww(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc2e41",
   "metadata": {},
   "source": [
    "## Section 2 -- Tooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb60ff51",
   "metadata": {},
   "source": [
    "###  First we show how tools work and how is it done natively\n",
    "\n",
    "Create a set of helper function\n",
    "\n",
    "we will create a set of functions which we can the re use in our application\n",
    "1. We will need to create a prompt template. This template helps Bedrock models understand the tools and how to invoke them.\n",
    "2. Create a method to read the available tools and add it to the prompt being used to invoke Claude\n",
    "3. Call function which will be responsbile to actually invoke the function with the `right` parameters\n",
    "4. Format Results for helping the Model leverage the results for summarization\n",
    "5. `Add to prompt`. The result which come back need to be added to the the prompt and model invoked again to get the right results\n",
    "\n",
    "[See this notebook for more details](https://github.com/aws-samples/amazon-bedrock-samples/blob/main/rag-solutions/rag-foundations-workshop/notebooks/05_agent_based_text_generation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73fdb2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8103b5",
   "metadata": {},
   "source": [
    "### Add Tools\n",
    "\n",
    "Recursively add the available tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0d72b4",
   "metadata": {},
   "source": [
    "### With LangChain\n",
    "\n",
    "Now use langchain and annotations to create the tools and invoke the functions. we will call weather api. The first tool will be used to look up the Latitude and second to get the weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f64328fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from langchain.tools import tool\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain import LLMMathChain\n",
    "\n",
    "@tool (\"get_lat_long\")\n",
    "def get_lat_long(place: str) -> dict:\n",
    "    \"\"\"Returns the latitude and longitude for a given place name as a dict object of python.\"\"\"\n",
    "    url = \"https://nominatim.openstreetmap.org/search\"\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    params = {'q': place, 'format': 'json', 'limit': 1}\n",
    "    response = requests.get(url, params=params,headers=headers).json()\n",
    "\n",
    "    if response:\n",
    "        lat = response[0][\"lat\"]\n",
    "        lon = response[0][\"lon\"]\n",
    "        return {\"latitude\": lat, \"longitude\": lon}\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "@tool (\"get_weather\")\n",
    "def get_weather(latitude: str, longitude: str) -> dict:\n",
    "  \"\"\"Returns weather data for a given latitude and longitude.\"\"\"\n",
    "  url = f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current_weather=true\"\n",
    "  response = requests.get(url)\n",
    "  print_ww(f\"get_weather:tool:invoked::response={response}:\")\n",
    "  return response.json()\n",
    "\n",
    "#get_weather_tool = StructuredTool.from_function(get_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95565ca0",
   "metadata": {},
   "source": [
    "### Simple example of invoking Maths chain "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbe6dab",
   "metadata": {},
   "source": [
    "This next cell is to test a simple tool like Maths calculator. The response of the Bedrock models based on your constructed input. Note that we have not instrumented output to call the actual functions, but this should give you an idea on how Claude's output can be parsed and the corresponding functions can be subsequently called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509bfb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain import LLMMathChain\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate,HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "#from langchain_openai import ChatOpenAI\n",
    "\n",
    "#from langchain.utilities import SerpAPIWrapper\n",
    "\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" #\"anthropic.claude-v2\"\n",
    "math_chain_llm = BedrockChat(model_id=modelId, model_kwargs={\"temperature\":0,\"stop_sequences\" : [\"```output\"]})\n",
    "\n",
    "tools_list = [] #load_tools([\"serpapi\"], llm=react_agent_llm)\n",
    "\n",
    "llm_math_chain = LLMMathChain(llm=math_chain_llm, verbose=True)\n",
    "\n",
    "llm_math_chain.llm_chain.prompt.template = \"\"\"Human: Given a question with a math problem, provide only a single line mathematical expression that solves the problem in the following format. Don't solve the expression only create a parsable expression.\n",
    "```text\n",
    "${{single line mathematical expression that solves the problem}}\n",
    "```\n",
    "\n",
    "Assistant:\n",
    " Here is an example response with a single line mathematical expression for solving a math problem:\n",
    "```text\n",
    "37593**(1/5)\n",
    "```\n",
    "\n",
    "Human: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "llm_math_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e26a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_math_chain({'question': 'text 4**5'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e3e22",
   "metadata": {},
   "source": [
    "### Add the functions or tools as we have defined to create our `Agent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "445b9ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_list = [get_lat_long,get_weather]\n",
    "\n",
    "tools_list.append(Tool.from_function(func=llm_math_chain.run,name=\"Calculator\",description=\"Useful for when you need to answer questions about math.\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc6179",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tools_s in tools_list:\n",
    "    print_ww(f\"Tool:name={tools_s.name}::args={tools_s.args}:: discription={tools_s.description}::\")\n",
    "\n",
    "tools_list[2].invoke({\"input\":(4,5)}) # test the maths calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3301e5",
   "metadata": {},
   "source": [
    "### Tooling and Agents\n",
    "\n",
    "**Use the Default prompt template**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ee654",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain import LLMMathChain\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate,HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "#from langchain.utilities import SerpAPIWrapper\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" #\"anthropic.claude-v2\"\n",
    "react_agent_llm = BedrockChat(\n",
    "    model_id=modelId,\n",
    "    client=bedrock_runtime,\n",
    "    #model_kwargs={\"max_tokens_to_sample\": 100},\n",
    "    model_kwargs={\"temperature\": 0.1},\n",
    ")\n",
    "\n",
    "tools_list = [get_lat_long,get_weather]\n",
    "\n",
    "tools_list.append(Tool.from_function(func=llm_math_chain.run,name=\"Calculator\",description=\"Useful for when you need to answer questions about math.\",))\n",
    "\n",
    "\n",
    "\n",
    "def ask_agent(question, llm):\n",
    "    react_agent = initialize_agent(\n",
    "        tools_list, \n",
    "        react_agent_llm, \n",
    "        agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, #SELF_ASK_WITH_SEARCH,# CONVERSATIONAL_REACT_DESCRIPTION, # ZERO_SHOT_REACT_DESCRIPTION, \n",
    "        verbose=True,\n",
    "        max_iteration=4,\n",
    "        return_intermediate_steps=True,\n",
    "        max_execution_time=60,\n",
    "        #    handle_parsing_errors=True,\n",
    "        #prompt = prompt_template\n",
    "    )\n",
    "    print_ww(f\"The prompt:template:used:was ---- > \\n\\n{react_agent.agent.llm_chain.prompt}\\n\")\n",
    "    \n",
    "    \n",
    "    return react_agent(question)\n",
    "\n",
    "ask_agent(\"can you check the weather in Marysville WA for me?\", react_agent_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfeeaf1",
   "metadata": {},
   "source": [
    "### Tooling with Custom template\n",
    "\n",
    "**Now Use a custom prompt template to create the Agents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c829b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain import LLMMathChain\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate,HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate,PromptTemplate\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" \n",
    "react_agent_llm = BedrockChat(\n",
    "    model_id=modelId,\n",
    "    client=bedrock_runtime,\n",
    "    model_kwargs={\"temperature\": 0.1},\n",
    ")\n",
    "\n",
    "tools_list = [get_lat_long,get_weather]\n",
    "\n",
    "tools_list.append(Tool.from_function(func=llm_math_chain.run,name=\"Calculator\",description=\"Useful for when you need to answer questions about math.\",))\n",
    "\n",
    "\n",
    "prompt_template_sys = \"\"\"\n",
    "\n",
    "Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do, Also try to follow steps mentioned above\n",
    "Action: the action to take, should be one of [ \"get_lat_long\", \"get_weather\", \"Calculator\"]\n",
    "Action Input: the input to the action\\nObservation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Assistant:\n",
    "{agent_scratchpad}'\n",
    "\n",
    "\"\"\"\n",
    "messages=[\n",
    "    SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'input'], template=prompt_template_sys)), \n",
    "    HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))\n",
    "]\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "print_ww(f\"from:messages:prompt:template:{chat_prompt_template}\")\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate(\n",
    "    input_variables=['agent_scratchpad', 'input'], \n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "\n",
    "react_agent = initialize_agent(\n",
    "    tools_list, \n",
    "    react_agent_llm, \n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, #SELF_ASK_WITH_SEARCH,# CONVERSATIONAL_REACT_DESCRIPTION, # ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True,\n",
    "    max_iteration=4,\n",
    "    return_intermediate_steps=True,\n",
    "    #    handle_parsing_errors=True,\n",
    "    prompt = chat_prompt_template\n",
    ")\n",
    "\n",
    "react_agent.invoke({\"input\":\"can you check the weather in Marysville WA for me?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ff5af",
   "metadata": {},
   "source": [
    "## Section 2) Leverage the create_tool_calling_agent API - \n",
    "\n",
    "**for bind_tools function**\n",
    "\n",
    "This patches the bind_tools missing api to allow for tool creation. This is just a sample of how it can be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90b576b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- https://github.com/langchain-ai/langchain/blob/master/libs/partners/openai/langchain_openai/chat_models/base.py\n",
    "\n",
    "#- bedrockChat -- https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/chat_models/bedrock.py\n",
    "\n",
    "#- anthorpic https://github.com/langchain-ai/langchain/blob/master/libs/partners/anthropic/langchain_anthropic/chat_models.py  -- #3 has the base chat model which has the bind_tools\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "from typing import (\n",
    "    Any,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    List,\n",
    "    Literal,\n",
    "    Type,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "PYTHON_TO_JSON_TYPES = {\n",
    "    \"str\": \"string\",\n",
    "    \"int\": \"integer\",\n",
    "    \"float\": \"number\",\n",
    "    \"bool\": \"boolean\",\n",
    "}\n",
    "\n",
    "SYSTEM_PROMPT_FORMAT = \"\"\"In this environment you have access to a set of tools you can use to answer the user's question.\n",
    "\n",
    "You may call them like this:\n",
    "<function_calls>\n",
    "<invoke>\n",
    "<tool_name>$TOOL_NAME</tool_name>\n",
    "<parameters>\n",
    "<$PARAMETER_NAME>$PARAMETER_VALUE</$PARAMETER_NAME>\n",
    "...\n",
    "</parameters>\n",
    "</invoke>\n",
    "</function_calls>\n",
    "\n",
    "Here are the tools available:\n",
    "<tools>\n",
    "{formatted_tools}\n",
    "</tools>\"\"\"  # noqa: E501\n",
    "\n",
    "TOOL_FORMAT = \"\"\"<tool_description>\n",
    "<tool_name>{tool_name}</tool_name>\n",
    "<description>{tool_description}</description>\n",
    "<parameters>\n",
    "{formatted_parameters}\n",
    "</parameters>\n",
    "</tool_description>\"\"\"\n",
    "\n",
    "TOOL_PARAMETER_FORMAT = \"\"\"<parameter>\n",
    "<name>{parameter_name}</name>\n",
    "<type>{parameter_type}</type>\n",
    "<description>{parameter_description}</description>\n",
    "</parameter>\"\"\"\n",
    "\n",
    "def _get_type(parameter: Dict[str, Any]) -> str:\n",
    "    if \"type\" in parameter:\n",
    "        return parameter[\"type\"]\n",
    "    if \"anyOf\" in parameter:\n",
    "        return json.dumps({\"anyOf\": parameter[\"anyOf\"]})\n",
    "    if \"allOf\" in parameter:\n",
    "        return json.dumps({\"allOf\": parameter[\"allOf\"]})\n",
    "    return json.dumps(parameter)\n",
    "\n",
    "def get_system_message(tools: List[AnthropicTool]) -> str:\n",
    "    tools_data: List[Dict] = [\n",
    "        {\n",
    "            \"tool_name\": tool[\"name\"],\n",
    "            \"tool_description\": tool[\"description\"],\n",
    "            \"formatted_parameters\": \"\\n\".join(\n",
    "                [\n",
    "                    TOOL_PARAMETER_FORMAT.format(\n",
    "                        parameter_name=name,\n",
    "                        parameter_type=_get_type(parameter),\n",
    "                        parameter_description=parameter.get(\"description\"),\n",
    "                    )\n",
    "                    for name, parameter in tool[\"input_schema\"][\"properties\"].items()\n",
    "                ]\n",
    "            ),\n",
    "        }\n",
    "        for tool in tools\n",
    "    ]\n",
    "    tools_formatted = \"\\n\".join(\n",
    "        [\n",
    "            TOOL_FORMAT.format(\n",
    "                tool_name=tool[\"tool_name\"],\n",
    "                tool_description=tool[\"tool_description\"],\n",
    "                formatted_parameters=tool[\"formatted_parameters\"],\n",
    "            )\n",
    "            for tool in tools_data\n",
    "        ]\n",
    "    )\n",
    "    return SYSTEM_PROMPT_FORMAT.format(formatted_tools=tools_formatted)\n",
    "\n",
    "class _AnthropicToolUse(TypedDict):\n",
    "    type: Literal[\"tool_use\"]\n",
    "    name: str\n",
    "    input: dict\n",
    "    id: str\n",
    "    \n",
    "class AnthropicTool(TypedDict):\n",
    "    name: str\n",
    "    description: str\n",
    "    input_schema: Dict[str, Any]\n",
    "\n",
    "class ToolsBedrockChat(BedrockChat):\n",
    "\n",
    "    system_prompt_with_tools: str = \"\"\n",
    "    def convert_to_anthropic_tool(self,\n",
    "        tool: Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool],\n",
    "    ) -> AnthropicTool:\n",
    "        # already in Anthropic tool format\n",
    "        if isinstance(tool, dict) and all(\n",
    "            k in tool for k in (\"name\", \"description\", \"input_schema\")\n",
    "        ):\n",
    "            return AnthropicTool(tool)  # type: ignore\n",
    "        else:\n",
    "            formatted = convert_to_openai_tool(tool)[\"function\"]\n",
    "            return AnthropicTool(\n",
    "                name=formatted[\"name\"],\n",
    "                description=formatted[\"description\"],\n",
    "                input_schema=formatted[\"parameters\"],\n",
    "            )\n",
    "\n",
    "\n",
    "    def _tools_in_params(self,params: dict) -> bool:\n",
    "        return \"tools\" in params or (\n",
    "            \"extra_body\" in params and params[\"extra_body\"].get(\"tools\")\n",
    "        )\n",
    "    \n",
    "    def set_system_prompt_with_tools(self, xml_tools_system_prompt: str) -> None:\n",
    "        \"\"\"Workaround to bind. Sets the system prompt with tools\"\"\"\n",
    "        self.system_prompt_with_tools = xml_tools_system_prompt\n",
    "\n",
    "    def bind_tools(\n",
    "        self,\n",
    "        tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],\n",
    "        *,\n",
    "        tool_choice: Optional[Union[dict, str, Literal[\"auto\", \"none\"], bool]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Runnable[LanguageModelInput, BaseMessage]:\n",
    "        \"\"\"Bind tool-like objects to this chat model.\n",
    "\n",
    "        Assumes model has a tool calling API.\n",
    "\n",
    "        Args:\n",
    "            tools: A list of tool definitions to bind to this chat model.\n",
    "                Can be  a dictionary, pydantic model, callable, or BaseTool. Pydantic\n",
    "                models, callables, and BaseTools will be automatically converted to\n",
    "                their schema dictionary representation.\n",
    "            tool_choice: Which tool to require the model to call.\n",
    "                Must be the name of the single provided function or\n",
    "                \"auto\" to automatically determine which function to call\n",
    "                (if any), or a dict of the form:\n",
    "                {\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}.\n",
    "            **kwargs: Any additional parameters to pass to the\n",
    "                :class:`~langchain.runnable.Runnable` constructor.\n",
    "        \"\"\"\n",
    "        provider = self._get_provider()\n",
    "\n",
    "        if provider == \"anthropic\":\n",
    "            formatted_tools = [self.convert_to_anthropic_tool(tool) for tool in tools]\n",
    "            system_formatted_tools = get_system_message(formatted_tools)\n",
    "            self.set_system_prompt_with_tools(system_formatted_tools)\n",
    "            # llm_with_tools = self.bind(tools=system_formatted_tools, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def new_func(self):\n",
    "        print(\"pass\")\n",
    "    \n",
    "\n",
    "react_agent_llm = ToolsBedrockChat(\n",
    "    model_id=modelId,\n",
    "    client=bedrock_runtime,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8af813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "prompt_template_sys = \"\"\"\n",
    "\n",
    "Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do, Also try to follow steps mentioned above\n",
    "Action: the action to take, should be one of [ \"get_lat_long\", \"get_weather\", \"Calculator\"]\n",
    "Action Input: the input to the action\\nObservation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Assistant:\n",
    "{agent_scratchpad}'\n",
    "\n",
    "\"\"\"\n",
    "messages=[\n",
    "    SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'input'], template=prompt_template_sys)), \n",
    "    HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))\n",
    "]\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "print_ww(f\"from:messages:prompt:template:{chat_prompt_template}\")\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate(\n",
    "    input_variables=['agent_scratchpad', 'input'], \n",
    "    messages=messages\n",
    ")\n",
    "print_ww(f\"Crafted::prompt:template:{chat_prompt_template}\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#react_agent_llm.bind_tools = custom_bind_func\n",
    "\n",
    "# Construct the Tools agent\n",
    "react_agent = create_tool_calling_agent(react_agent_llm, tools_list,chat_prompt_template)\n",
    "agent_executor = AgentExecutor(agent=react_agent, tools=tools_list, verbose=True)\n",
    "agent_executor.invoke({\"input\": \"can you check the weather in Marysville WA for me?\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed6fc3e",
   "metadata": {},
   "source": [
    "## Section 3 Use the Langchain-AWS classes \n",
    "These classes having all thr latest api's and working correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6307a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" \n",
    "chat = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    model_kwargs={\"temperature\": 0.1},\n",
    "    client=bedrock_runtime\n",
    ")\n",
    "\n",
    "import requests\n",
    "\n",
    "from langchain.tools import tool\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain import LLMMathChain\n",
    "\n",
    "@tool (\"get_lat_long\")\n",
    "def get_lat_long(place: str) -> dict:\n",
    "    \"\"\"Returns the latitude and longitude for a given place name as a dict object of python.\"\"\"\n",
    "    url = \"https://nominatim.openstreetmap.org/search\"\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    params = {'q': place, 'format': 'json', 'limit': 1}\n",
    "    response = requests.get(url, params=params, headers=headers).json()\n",
    "\n",
    "    if response:\n",
    "        lat = response[0][\"lat\"]\n",
    "        lon = response[0][\"lon\"]\n",
    "        return {\"latitude\": lat, \"longitude\": lon}\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "@tool (\"get_weather\")\n",
    "def get_weather(latitude: str, longitude: str) -> dict:\n",
    "  \"\"\"Returns weather data for a given latitude and longitude.\"\"\"\n",
    "  url = f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current_weather=true\"\n",
    "  response = requests.get(url)\n",
    "  print_ww(f\"get_weather:tool:invoked::response={response}:\")\n",
    "  return response.json()\n",
    "\n",
    "\n",
    "\n",
    "llm_with_tools = chat.bind_tools([get_weather,get_lat_long])\n",
    "print_ww(llm_with_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebde4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.human import HumanMessage\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"what is the weather like in San Francisco\"\n",
    "    )\n",
    "]\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6939ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "react_agent_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=bedrock_runtime,\n",
    "    #model_kwargs={\"max_tokens_to_sample\": 100},\n",
    "    #model_kwargs={\"temperature\": 0.1},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcce0c09",
   "metadata": {},
   "source": [
    "#### Create your own template with langchain-aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d3f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "prompt_template_sys = \"\"\"\n",
    "\n",
    "Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do, Also try to follow steps mentioned above\n",
    "Action: the action to take, should be one of [ \"get_lat_long\", \"get_weather\", \"Calculator\"]\n",
    "Action Input: the input to the action\\nObservation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Assistant:\n",
    "{agent_scratchpad}'\n",
    "\n",
    "\"\"\"\n",
    "messages=[\n",
    "    SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'input'], template=prompt_template_sys)), \n",
    "    HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))\n",
    "]\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "print_ww(f\"from:messages:prompt:template:{chat_prompt_template}\")\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate(\n",
    "    input_variables=['agent_scratchpad', 'input'], \n",
    "    messages=messages\n",
    ")\n",
    "print_ww(f\"Crafted::prompt:template:{chat_prompt_template}\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#react_agent_llm.bind_tools = custom_bind_func\n",
    "\n",
    "# Construct the Tools agent\n",
    "react_agent = create_tool_calling_agent(react_agent_llm, tools_list,chat_prompt_template)\n",
    "agent_executor = AgentExecutor(agent=react_agent, tools=tools_list, verbose=True)\n",
    "agent_executor.invoke({\"input\": \"can you check the weather in Marysville WA for me?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a71d324",
   "metadata": {},
   "source": [
    "#### Creating the chain of Agents + tools manually\n",
    "\n",
    "If you want to create the Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b40bc910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnablePassthrough\n",
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "from langchain.agents.format_scratchpad.tools import (\n",
    "    format_to_tool_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.tools import ToolsAgentOutputParser\n",
    "\n",
    "llm_with_tools = react_agent_llm.bind_tools(tools_list)\n",
    "\n",
    "conversational_agent = (\n",
    "        RunnablePassthrough.assign(\n",
    "            agent_scratchpad=lambda x: format_to_tool_messages(x[\"intermediate_steps\"])\n",
    "        )\n",
    "        | chat_prompt_template\n",
    "        | llm_with_tools\n",
    "        | ToolsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c88ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_t = conversational_agent.invoke({\"input\": \"What is Amazon SageMaker Clarify?\", \"intermediate_steps\": []})\n",
    "\n",
    "print_ww(output_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48a0e8-147d-4525-a6b2-68a09af1b2c4",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "In this notebook we showed some basic examples of leveraging tools and agents when invoking Amazon Bedrock models using the AWS Python SDK. You're now ready to explore the other labs to dive deeper on different use-cases and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5733f3e8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "trainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
